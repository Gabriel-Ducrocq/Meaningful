{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from datetime import datetime \n",
    "%load_ext autotime\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 228 ms\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 379 ms\n"
     ]
    }
   ],
   "source": [
    "def softmax_pooling(x):\n",
    "    coefs = tf.nn.softmax(x, dim = -1)\n",
    "    softmax_pool = tf.reduce_sum(tf.multiply(coefs, x), axis = -1)\n",
    "    return softmax_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 968 ms\n"
     ]
    }
   ],
   "source": [
    "class PhysicalNet: \n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, output_size, keep_prob = 0.9, stddev = 0.0001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.init_biases()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        with tf.variable_scope(\"phys_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                if i == 0:\n",
    "                    #W = tf.Variable(tf.random_normal([self.layer_sizes[i], self.input_size], stddev = self.stddev), name = \"weight_\" + str(i))\n",
    "                    W = tf.get_variable(\"weight_\" + str(i), shape=[self.layer_sizes[i], self.input_size],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    tf.summary.histogram('phys_net_weight_'+ str(i), W)\n",
    "                elif i != (self.nb_layers - 1):\n",
    "                    W = tf.get_variable(\"weight_\" + str(i), shape=[self.layer_sizes[(i+1)], self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    #W = tf.Variable(tf.random_normal([self.layer_sizes[(i+1)], self.layer_sizes[i]], stddev = self.stddev), name = \"weight_\" + str(i))\n",
    "                    tf.summary.histogram('phys_net_weight_'+ str(i), W)\n",
    "                else:\n",
    "                    W = tf.get_variable(\"weight_\" + str(i), shape=[self.output_size, self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    #W = tf.Variable(tf.random_normal([self.output_size, self.layer_sizes[i]], stddev = self.stddev), name = \"weight_\" + str(i))\n",
    "                    tf.summary.histogram('phys_net_weight_'+ str(i), W)\n",
    "\n",
    "                self.Weights.append(W)\n",
    "            \n",
    "    def init_biases(self):\n",
    "        with tf.variable_scope(\"phys_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                #B = tf.Variable(tf.random_normal([self.layer_sizes[i], 1], stddev = self.stddev), name = \"bias_\" + str(i))\n",
    "                B = tf.get_variable(\"bias_\" + str(i), shape=[self.layer_sizes[i], 1],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                tf.summary.histogram('phys_net_bias_'+ str(i), B)\n",
    "                self.Biases.append(B)\n",
    "            \n",
    "            \n",
    "    def compute_output(self, x): ## ADD THE DROPOUTS ## ADD THE SOFTMAX AT THE END ?\n",
    "            for i in range(self.nb_layers):\n",
    "                W = self.Weights[i]\n",
    "                b = self.Biases[i]\n",
    "                if i != (self.nb_layers - 1):\n",
    "                    x = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "                else:\n",
    "                    x = tf.nn.elu(tf.matmul(W, x) + b)\n",
    "            \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 842 ms\n"
     ]
    }
   ],
   "source": [
    "class CommunicationNet: ## ADD THE MEMORY !! \n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, keep_prob = 0.9, memory_size = 32,\n",
    "                 stddev_epsilon = 0.35, output_size = 256, stddev = 0.0001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.memory_size = memory_size\n",
    "        self.stddev_epsilon = stddev_epsilon\n",
    "        self.output_size = output_size\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        self.Weight_read_mem = tf.Variable(tf.random_normal([self.layer_sizes[(self.nb_layers-1)], self.memory_size]\n",
    "                                                            ,stddev = self.stddev))\n",
    "        \n",
    "        self.init_weights()    \n",
    "        self.init_biases()\n",
    "        self.def_delta_mem()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        with tf.variable_scope(\"com_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                if i == 0:\n",
    "                    W = tf.get_variable(\"com_net_weight_\" + str(i), shape=[self.layer_sizes[i], self.input_size],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    #W = tf.Variable(tf.random_normal([self.layer_sizes[i], self.input_size], stddev = self.stddev), name = \"weight_\" + str(i))\n",
    "                    tf.summary.histogram('com_net_weight_'+ str(i), W)\n",
    "                elif i != (self.nb_layers - 1):\n",
    "                    W = tf.get_variable(\"com_net_weight_\" + str(i), shape=[self.layer_sizes[(i+1)], self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    #W = tf.Variable(tf.random_normal([self.layer_sizes[(i+1)], self.layer_sizes[i]],stddev = self.stddev), name = \"weight_\" + str(i))\n",
    "                    tf.summary.histogram('com_net_weight_'+ str(i), W)\n",
    "                else:\n",
    "                    W = tf.get_variable(\"com_net_weight_\" + str(i), shape=[self.output_size, self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    #W = tf.Variable(tf.random_normal([self.output_size, self.layer_sizes[i]],stddev = self.stddev), name = \"weight_\" + str(i))\n",
    "                    tf.summary.histogram('com_net_weight_'+ str(i), W)\n",
    "\n",
    "                self.Weights.append(W)\n",
    "       \n",
    "    \n",
    "    def init_biases(self):\n",
    "        with tf.variable_scope(\"com_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                #B = tf.Variable(tf.random_normal([self.layer_sizes[i], 1],stddev = self.stddev), name = \"bias_\" + str(i))\n",
    "                B = tf.Variable(tf.zeros([self.layer_sizes[i], 1]), name = \"com_net_bias_\" + str(i))\n",
    "                tf.summary.histogram('com_net_bias_'+ str(i), B)\n",
    "                self.Biases.append(B)\n",
    "       \n",
    "    \n",
    "    def def_delta_mem(self):\n",
    "        #self.W_mem = tf.Variable(tf.random_normal(shape =[self.memory_size,self.output_size],stddev = self.stddev), name = \"weight_mem_com\")\n",
    "        self.W_mem = tf.get_variable(\"weight_mem_com\" , shape=[self.memory_size,self.output_size],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "        self.b_mem = tf.Variable(tf.zeros(shape = [self.memory_size, 1]), name = \"bias_mem_com\")\n",
    "\n",
    "        \n",
    "    def compute_output(self, x, memory):## ADD THE DROPOUTS AND SOFTMAX ?\n",
    "        for i in range(self.nb_layers):\n",
    "            W = self.Weights[i]\n",
    "            b = self.Biases[i]\n",
    "            if i != (self.nb_layers - 1):\n",
    "                x = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "            else:\n",
    "                x = tf.nn.elu(tf.matmul(W, x) + tf.matmul(self.Weight_read_mem, memory) + b)\n",
    "                \n",
    "            \n",
    "        delta_mem = tf.add(tf.matmul(self.W_mem, x),self.b_mem)\n",
    "        return x, delta_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 581 ms\n"
     ]
    }
   ],
   "source": [
    "class LastNet: ## ADD THE MEMORY !! The memory initialization is random ==> set it 0\n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, keep_prob = 0.9, memory_size = 32, \n",
    "                 stddev_epsilon = 0.35, output_size = 24, stddev = 0.0001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.memory_size = memory_size\n",
    "        self.stddev_epsilon = stddev_epsilon\n",
    "        self.output_size = output_size\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        self.Weight_read_mem = tf.Variable(tf.random_normal([self.output_size, self.memory_size],stddev = self.stddev))\n",
    "        \n",
    "        self.init_weights()    \n",
    "        self.init_biases()\n",
    "        self.def_delta_mem()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        with tf.variable_scope(\"last_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                if i == 0:\n",
    "                    W = tf.get_variable(\"lastnet_weight_\" + str(i), shape=[self.layer_sizes[i], self.input_size],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    #W = tf.Variable(tf.random_normal([self.layer_sizes[i], self.input_size],stddev = self.stddev), name = \"weight_\" + str(i))\n",
    "                    tf.summary.histogram('last_net_weight_'+ str(i), W)\n",
    "                elif i != (self.nb_layers - 1):\n",
    "                    W = tf.get_variable(\"last_net_weight_\" + str(i), shape=[self.layer_sizes[(i+1)], self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    #W = tf.Variable(tf.random_normal([self.layer_sizes[(i+1)], self.layer_sizes[i]],stddev = self.stddev), name = \"weight_\" + str(i))\n",
    "                    tf.summary.histogram('last_net_weight_'+ str(i), W)\n",
    "                else:\n",
    "                    W = tf.get_variable(\"last_net_weight_\" + str(i), shape=[self.output_size, self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    #W = tf.Variable(tf.random_normal([self.output_size, self.layer_sizes[i]],stddev = self.stddev), name = \"weight_\" + str(i))\n",
    "                    tf.summary.histogram('last_net_weight_'+ str(i), W)\n",
    "\n",
    "                self.Weights.append(W)\n",
    "            \n",
    "    def init_biases(self):\n",
    "        with tf.variable_scope(\"last_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                if i != (self.nb_layers - 1):\n",
    "                    #B = tf.Variable(tf.random_normal([self.layer_sizes[i+1], 1],stddev = self.stddev), name = \"bias_\" + str(i))\n",
    "                    B = tf.Variable(tf.zeros([self.layer_sizes[i], 1]), name = \"last_net_bias_\" + str(i))\n",
    "                    tf.summary.histogram('last_net_bias_'+ str(i), B)\n",
    "                else:\n",
    "                    #B = tf.Variable(tf.random_normal([self.output_size, 1], stddev = self.stddev), name = \"bias_\" + str(i))\n",
    "                    B = tf.Variable(tf.zeros([self.output_size, 1]), name = \"last_net_bias_\" + str(i))\n",
    "                    tf.summary.histogram('last_net_bias_'+ str(i), B)\n",
    "\n",
    "                self.Biases.append(B)\n",
    "\n",
    "        \n",
    "    def def_delta_mem(self):\n",
    "        #self.W_mem = tf.Variable(tf.random_normal(shape =[self.memory_size,self.output_size],stddev = self.stddev),\n",
    "        #                        name = \"weight_mem_last\")\n",
    "        self.W_mem = tf.get_variable(\"weight_mem_last\", shape=[self.memory_size,self.output_size],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "        self.b_mem = tf.Variable(tf.zeros(shape = [self.memory_size, 1]),\n",
    "                                name = \"bias_mem_last\")\n",
    "        \n",
    "    def compute_output(self, x, memory):## ADD THE DROPOUTS !!! REMOVE THE SOFTMAX OF THE LAST LAYER !!!\n",
    "        for i in range(self.nb_layers):\n",
    "            W = self.Weights[i]\n",
    "            b = self.Biases[i]\n",
    "            if i != (self.nb_layers - 1):\n",
    "                x = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "            else:\n",
    "                x = tf.nn.elu(tf.matmul(W, x) + tf.matmul(self.Weight_read_mem, memory) + b)\n",
    "               \n",
    "        delta_mem = tf.add(tf.matmul(self.W_mem, x),self.b_mem)\n",
    "        return x, delta_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "class Policy_Phys:\n",
    "    \n",
    "    def __init__(self, nb_agent, nb_landmark, hidden_layer_size = 256, env_dim = 2, \n",
    "                 batch_size = 1024, stddev_phys_output = 0.0001):\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.batch_size = batch_size\n",
    "        self.env_dim = env_dim\n",
    "        self.nb_agent = nb_agent\n",
    "        self.nb_landmark = nb_landmark\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.init_all()\n",
    "        \n",
    "    def init_phys_module(self):\n",
    "        self.network_phys = PhysicalNet([self.hidden_layer_size, self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                              self.env_dim, self.hidden_layer_size)\n",
    "\n",
    "\n",
    "    def compute_output(self, x_agent):\n",
    "        output = []\n",
    "        for x in x_agent:\n",
    "            output.append(tf.reshape(self.network_phys.compute_output(x), [256, -1, 1]))\n",
    "            \n",
    "        all_phys_output = tf.concat(output, axis = 2)\n",
    "        self.PhiX = softmax_pooling(all_phys_output)\n",
    "        return self.PhiX\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_phys_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 321 ms\n"
     ]
    }
   ],
   "source": [
    "class Policy_Utterance:\n",
    "    \n",
    "    def __init__(self, nb_agent, goal_size, vocabulary_size = 20, \n",
    "                 hidden_layer_size = 256, memory_size = 32, temperature = 1, batch_size = 1024,\n",
    "                 stddev_phys_output = 0.0001):\n",
    "        self.size_goal = goal_size\n",
    "        self.nb_agent = nb_agent\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.memory_size = memory_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.goal = tf.placeholder(tf.float32, [self.size_goal, None])\n",
    "        \n",
    "        self.init_all()\n",
    "        \n",
    "\n",
    "    def init_com_module(self):## Les poids seront les mêmes pour tous les agents\n",
    "        self.network_com = CommunicationNet([self.hidden_layer_size, self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                             self.vocabulary_size)\n",
    "\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_com_module()\n",
    "\n",
    "    def compute_output(self, c_agent, mem_agent):\n",
    "        output = []\n",
    "        delta_mem = []\n",
    "        for c, mem in zip(c_agent, mem_agent):\n",
    "            o, m = self.network_com.compute_output(c, mem)\n",
    "            output.append(tf.reshape(o, [256, -1, 1]))\n",
    "            delta_mem.append(m)\n",
    "\n",
    "        all_comm_output = tf.concat(output, axis = 2)\n",
    "        PhiC = softmax_pooling(all_comm_output)\n",
    "        \n",
    "        return PhiC, delta_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 275 ms\n"
     ]
    }
   ],
   "source": [
    "class Policy_Last:\n",
    "    \n",
    "    def __init__(self, hidden_layer_size = 256, size_goal = 8, memory_size = 32, batch_size = 1024, \n",
    "                 stddev_phys_output = 0.0001, vocabulary_size = 20, env_dim = 2, temperature = 1):\n",
    "        self.temperature = temperature\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.env_dim = env_dim\n",
    "        self.vocabulary_size = vocabulary_size \n",
    "        self.batch_size = batch_size\n",
    "        self.memory_size = memory_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.size_goal = size_goal\n",
    "        self.init_all()\n",
    "\n",
    "    def init_last_module(self):\n",
    "        inp_size = (2*self.hidden_layer_size + self.size_goal)\n",
    "        out_size = self.vocabulary_size + 2*self.env_dim\n",
    "        self.last_net = LastNet([self.hidden_layer_size, self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                inp_size, output_size = out_size)\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_last_module()\n",
    "\n",
    "    def sample_utterance(self, output):## Vérifier qu'on prend un bon slice sur l'output\n",
    "        u = -tf.log(-tf.log(tf.random_uniform(shape = [self.vocabulary_size, self.batch_size],dtype=tf.float32)))\n",
    "        utterance_output = tf.slice(output, [2*self.env_dim, 0], [self.vocabulary_size, self.batch_size])\n",
    "        gumbel = tf.exp((utterance_output + u)/self.temperature)\n",
    "        denoms = tf.reduce_sum(gumbel, axis = 0)\n",
    "        utterance = gumbel/denoms\n",
    "        return utterance\n",
    "        \n",
    "    def sample_phys(self, output):\n",
    "        u = tf.random_normal(shape = [2*self.env_dim, self.batch_size],dtype=tf.float32, stddev = self.stddev_phys_output)\n",
    "        o = tf.add(tf.slice(output, [0, 0], [2*self.env_dim, self.batch_size]), u)\n",
    "        sample_move = tf.slice(o, [0, 0], [self.env_dim, self.batch_size])\n",
    "        sample_gaze  = tf.slice(o, [self.env_dim, 0], [self.env_dim, self.batch_size])\n",
    "        return sample_move, sample_gaze\n",
    "            \n",
    "    def compute_output(self, PhiX, PhiC, last_mem, goal):\n",
    "        Phi = tf.concat([PhiX, goal, PhiC], axis = 0)\n",
    "        output, memory = self.last_net.compute_output(Phi, last_mem)\n",
    "        utterance = self.sample_utterance(output)\n",
    "        move, gaze = self.sample_phys(output)\n",
    "        return move, gaze, utterance, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "class Policy:# Two memories per Agent: one for the communication module, the other one for the last module. Is it correct ?\n",
    "\n",
    "    def __init__(self,nb_agent, nb_landmark, goal_size, vocabulary_size = 20, hidden_layer_size = 256,\n",
    "                 memory_size = 32, temperature = 1, batch_size = 1024, stddev_phys_output = 0.0001, env_dim = 2,\n",
    "                 goal_type_size = 3, time_delta = 0.1, damping_coeff = 0.5):\n",
    "        self.nb_agent = nb_agent\n",
    "        self.goal_size = goal_size\n",
    "        self.nb_landmark = nb_landmark\n",
    "        self.env_dim = env_dim\n",
    "        self.goal_type_size = goal_type_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.memory_size = memory_size\n",
    "        self.time_delta = time_delta\n",
    "        self.damping_coeff = damping_coeff\n",
    "        \n",
    "        self.phys_module = Policy_Phys(self.nb_agent, self.nb_landmark)\n",
    "        self.utterance_module = Policy_Utterance(self.nb_agent, self.goal_size)\n",
    "        self.last_module = Policy_Last()\n",
    "        \n",
    "        self.list_PhiX = []\n",
    "        self.list_PhiC = []\n",
    "        \n",
    "        self.init_placeholders()\n",
    "         \n",
    "    #h_pos, h_utter, h_velocity, h_gaze, h_goal_location, h_goal_type, h_col, h_mem, h_last_mem \n",
    "    def init_placeholders(self):\n",
    "        self.h_pos = [tf.placeholder(tf.float32, shape = [self.env_dim, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_utterance = [tf.placeholder(tf.float32, shape = [self.vocabulary_size, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_velocity = [tf.placeholder(tf.float32, shape = [self.env_dim, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_gaze = [tf.placeholder(tf.float32, shape = [self.env_dim, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_goal_location = [tf.placeholder(tf.float32, shape = [self.env_dim, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_goal_type = [tf.placeholder(tf.float32, shape = [self.goal_type_size, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_col = [tf.placeholder(tf.float32, shape = [3, 1]) for i in range(self.nb_agent)]\n",
    "        self.h_mem = [tf.placeholder(tf.float32, shape = [self.memory_size, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_mem_last = [tf.placeholder(tf.float32, shape = [self.memory_size, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_name_targets = [tf.placeholder(tf.int32, shape = [1, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        \n",
    "        \n",
    "    def get_placeholders(self):\n",
    "        return [self.h_pos, self.h_utterance, self.h_velocity, self.h_gaze, self.h_goal_location, self.h_goal_type,\n",
    "        self.h_col, self.h_mem, self.h_mem_last, self.h_name_targets]\n",
    "    \n",
    "    def get_list_goals(self, list_name_targets):\n",
    "        list_goals = []\n",
    "        for i in range(self.nb_agent):\n",
    "            list_goals.append(self.init_goal_agent(i))\n",
    "            \n",
    "        return list_goals\n",
    "\n",
    "    def init_goal_agent(self, agent_number):\n",
    "        other_agents = tf.gather(self.h_col, self.h_name_targets[agent_number])\n",
    "        colors = tf.reshape(tf.concat(other_agents, axis = 1), [3, self.batch_size])\n",
    "        return tf.concat([self.h_goal_type[agent_number], self.h_goal_location[agent_number], colors], axis = 0)\n",
    "        \n",
    "    def compute_output(self, list_pos, list_utterances, list_goals, list_mem, list_last_mem):\n",
    "        list_moves = []\n",
    "        list_gazes = []\n",
    "        list_new_utterances = []\n",
    "        list_last_delta_mem = []\n",
    "        PhiX = self.phys_module.compute_output(list_pos)\n",
    "        PhiC, list_delta_mem = self.utterance_module.compute_output(list_utterances, list_mem)\n",
    "        self.list_PhiX.append(PhiX)\n",
    "        self.list_PhiC.append(PhiC)\n",
    "        for last_mem, goal in zip(list_last_mem, list_goals):\n",
    "            m, g, u, ldm = self.last_module.compute_output(PhiX, PhiC, last_mem, goal)\n",
    "            list_moves.append(m)\n",
    "            list_gazes.append(g)\n",
    "            list_new_utterances.append(u)\n",
    "            list_last_delta_mem.append(ldm)\n",
    "            \n",
    "        return list_moves, list_gazes, list_new_utterances, list_delta_mem, list_last_delta_mem\n",
    "    \n",
    "    def compute_reward_agents(self, new_pos, new_gaze, list_move, new_utterance, goal_locations, list_targets, goal_type): \n",
    "        ## Check the shuffle for the pos of agent and gaze is OK !!\n",
    "        rewards = []\n",
    "        ag_positions = []\n",
    "        ag_gazes = []\n",
    "        ag_goal_on_agent = []\n",
    "        ag_velocities = []\n",
    "        ag_utterances = []\n",
    "        ag_goal_locations = []\n",
    "        ag_goal_type = []\n",
    "        for i in range(self.nb_agent):## Useless, take list directly\n",
    "            ag_positions.append(new_pos[i])\n",
    "            ag_gazes.append(new_gaze[i])\n",
    "            ag_velocities.append(list_move[i])\n",
    "            ag_utterances.append(new_utterance[i])\n",
    "            ag_goal_locations.append(goal_locations[i])\n",
    "            ag_goal_type.append(goal_type[i])\n",
    "            \n",
    "        agent_positions = tf.stack(ag_positions, axis = 2)\n",
    "        agent_gazes = tf.stack(ag_gazes, axis = 2)\n",
    "        agent_velocities = tf.stack(ag_velocities, axis = 2)\n",
    "        agent_utterances = tf.stack(ag_utterances, axis = 2)\n",
    "        agent_goal_locations = tf.stack(ag_goal_locations, axis = 2)\n",
    "        agent_goal_type = tf.stack(ag_goal_type, axis = 2)\n",
    "        \n",
    "        for i in range(self.nb_agent):\n",
    "            name_target = tf.reshape(tf.gather_nd(list_targets[i], [[0,k] for k in range(self.batch_size)]), [self.batch_size, 1])\n",
    "            l1 = tf.concat([tf.constant([[0] for i in range(self.batch_size)]), tf.constant([[i] for i in range(self.batch_size)]),\n",
    "                           name_target], axis = 1) \n",
    "            l2 = tf.concat([tf.constant([[1] for i in range(self.batch_size)]), tf.constant([[i] for i in range(self.batch_size)]),\n",
    "                           name_target], axis = 1)            \n",
    "            l3 = [l1, l2]\n",
    "\n",
    "            position_target = tf.gather_nd(agent_positions, l3)\n",
    "            gaze_target = tf.gather_nd(agent_gazes, l3)\n",
    "            \n",
    "            own_velocity = tf.reshape(tf.slice(agent_velocities, [0, 0, i], [self.env_dim, self.batch_size, 1]), \n",
    "                                         [self.env_dim, self.batch_size])  \n",
    "            \n",
    "            own_gaze = tf.reshape(tf.slice(agent_gazes, [0, 0, i], [self.env_dim, self.batch_size, 1]), \n",
    "                                         [self.env_dim, self.batch_size])\n",
    "            \n",
    "            own_goal_location = tf.reshape(tf.slice(agent_goal_locations, [0, 0, i], [self.env_dim, self.batch_size, 1]), \n",
    "                                          [self.env_dim, self.batch_size])\n",
    "            \n",
    "            own_utterance = tf.reshape(tf.slice(agent_utterances, [0, 0, i], [self.vocabulary_size, self.batch_size, 1]), \n",
    "                                         [self.vocabulary_size, self.batch_size])\n",
    "            \n",
    "            own_goal_type = tf.reshape(tf.slice(agent_goal_type, [0, 0, i], [3, self.batch_size, 1]), \n",
    "                                          [3, self.batch_size])\n",
    "            \n",
    "            reward_agent = self.compute_reward_agent(position_target, gaze_target, own_velocity, own_gaze,\n",
    "                                                      own_utterance, own_goal_location, own_goal_type)\n",
    "            rewards.append(reward_agent)\n",
    "          \n",
    "        #Concat on another axis\n",
    "        rewards_batch = tf.reduce_mean(tf.concat(rewards, axis = 0), axis = 0)\n",
    "        return rewards_batch\n",
    "    \n",
    "    \n",
    "    def compute_reward_agent(self,goal_agent_pos, goal_agent_gaze, output_velocity, output_gaze, new_utterance, \n",
    "                             goal_location, goal_type): \n",
    "        ## Modifier la norme u, il s'agit de l'output\n",
    "        #du réseau, non pas de la position !\n",
    "        with tf.name_scope(\"reward_computation\"):\n",
    "            r1 = tf.reshape(tf.square(tf.norm(goal_agent_pos - goal_location, axis = 0)), [1, self.batch_size])\n",
    "            r2 = tf.reshape(tf.square(tf.norm(goal_agent_gaze - goal_location, axis = 0)), [1, self.batch_size])\n",
    "            utt_norm = tf.square(tf.norm(new_utterance, axis = 0))\n",
    "            u_norm = tf.square(tf.norm(tf.concat([output_velocity, output_gaze], axis = 0), axis = 0))\n",
    "            vec = tf.concat([r1, r2, tf.zeros([1,self.batch_size], tf.float32)], axis = 0)\n",
    "            v1 = tf.reduce_sum(tf.multiply(vec, goal_type), axis = 0)\n",
    "            r = -(v1 + utt_norm + u_norm)\n",
    "        return r\n",
    "    \n",
    "    def compute_new_state(self, tensor_new_utterance, tensor_new_velocity, tensor_new_gaze, tensor_memory_delta, \n",
    "                          tensor_memory_delta_last, tensor_pos, tensor_velocity, tensor_memory, tensor_memory_last):\n",
    "        ## ADD THE FORCES TO THE NEW VELOCITY !!\n",
    "        ## ADD GAUSSIAN NOISE TO THE MEMORY UPDATE !\n",
    "        new_pos = tensor_pos + tf.multiply(tensor_velocity,self.time_delta)\n",
    "        new_velocity = (tf.multiply(tensor_velocity, self.damping_coeff) + \n",
    "                                                  tf.multiply(tensor_new_velocity, self.time_delta))\n",
    "        new_gaze = tensor_new_gaze\n",
    "        new_memory = tensor_memory + tensor_memory_delta\n",
    "        new_memory_last = tensor_memory_last + tensor_memory_delta_last\n",
    "        new_utterance = tensor_new_utterance\n",
    "        \n",
    "        return new_pos, new_velocity, new_gaze, new_utterance, new_memory, new_memory_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 238 ms\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, name, name_target, vocabulary_size = 20, batch_size = 1024, env_dim = 2, goal_size = 8, \n",
    "                 memory_size = 32, time_delta = 0.1, nb_actions = 3, damping_coeff = 0.5):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.name_target = name_target\n",
    "        self.env_dim = env_dim\n",
    "        self.memory_size = memory_size\n",
    "        self.name = name\n",
    "        self.goal_size = goal_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "        #with tf.variable_scope(\"agent\" + str(self.name)):\n",
    "        #    self.pos = tf.placeholder(tf.float32, [self.env_dim, self.batch_size]) \n",
    "        #    self.velocity = tf.placeholder(tf.float32, [self.env_dim, self.batch_size])\n",
    "        #    self.gaze = tf.placeholder(tf.float32, [self.env_dim, self.batch_size])\n",
    "        #    self.utterance = tf.placeholder(tf.float32, [self.vocabulary_size, self.batch_size])\n",
    "\n",
    "        #   self.memory = tf.placeholder(tf.float32, [self.memory_size,self.batch_size])\n",
    "\n",
    "        #   self.memory_last = tf.placeholder(tf.float32, [self.memory_size, self.batch_size])\n",
    "    \n",
    "        #    self.tensor_goal_location = tf.placeholder(tf.float32, [self.env_dim, self.batch_size])\n",
    "        #    self.tensor_goal_type = tf.placeholder(tf.float32, [self.nb_actions, self.batch_size])\n",
    "        #    self.col = tf.placeholder(tf.float32, [3, 1])\n",
    "                                            \n",
    "    def get_position(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def get_velocity(self):\n",
    "        return self.velocity\n",
    "    \n",
    "    def get_gaze(self):\n",
    "        return self.gaze\n",
    "\n",
    "    def get_utterance(self):\n",
    "        return self.utterance\n",
    "                                        \n",
    "    def get_memory(self):\n",
    "        return self.memory\n",
    "    \n",
    "    def get_memory_last(self):\n",
    "        return self.memory_last\n",
    "    \n",
    "    def get_phys_state(self):\n",
    "        return (self.get_position(), self.get_velocity(), self.get_gaze(), self.get_col)\n",
    "    \n",
    "    def get_name_target(self):\n",
    "        return self.name_target\n",
    "    \n",
    "    def get_goal(self, other_ags):\n",
    "        other_agents = [other_ags[i].get_color() for i in self.name_target[0, :]]\n",
    "        colors = tf.concat(other_agents, axis = 1)\n",
    "        return tf.concat([self.tensor_goal_type, self.tensor_goal_location, colors], axis = 0)\n",
    "          \n",
    "    def get_all_iterations_variables(self):\n",
    "        return self.pos, self.velocity, self.gaze, self.utterance, self.memory, self.memory_last\n",
    "    \n",
    "    def get_color(self):\n",
    "        return self.col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 108 ms\n"
     ]
    }
   ],
   "source": [
    "class Environment:\n",
    "    # Use this class to instantiate an environment on N batches. All batches share the same structure, but not not the\n",
    "    # same goals.\n",
    "    def __init__(self, nb_agents = 3, nb_landmarks = 0, time_delta = 0.1, env_dim = 2, batch_size = 1024,\n",
    "                 goal_type_size = 3, damping_coef = 0.5, vocabulary_size = 20):\n",
    "        self.env_dim = env_dim\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.batch_size = batch_size\n",
    "        self.goal_type_size = goal_type_size\n",
    "        self.goal_size = self.goal_type_size + 3 + self.env_dim\n",
    "        self.nb_agents = nb_agents\n",
    "        self.nb_landmarks = nb_landmarks\n",
    "        self.time_delta = tf.constant([time_delta])\n",
    "        self.list_agents = []\n",
    "        self.list_phys_tensors = []\n",
    "        self.list_utter_tensors = []\n",
    "        self.list_mem_tensors = []\n",
    "        self.list_mem_last_tensors = []\n",
    "        self.list_goals_tensors = []\n",
    "        self.list_velocity_tensors = []\n",
    "        self.list_gaze_tensors = []\n",
    "        self.goal_type_tensors = []\n",
    "        self.col_tensors = []\n",
    "        self.goal_location_tensors = []\n",
    "        self.damping_coeff = damping_coef\n",
    "        self.enc = OneHotEncoder(n_values=self.goal_type_size, sparse=False)\n",
    "    \n",
    "        \n",
    "    def init_agents(self):\n",
    "        for i in range(self.nb_agents):\n",
    "            ag = Agent(name = i, name_target = self.name_of_targets[i])\n",
    "            self.list_agents.append(ag)\n",
    "            self.list_phys_tensors.append(ag.get_position())\n",
    "            self.list_utter_tensors.append(ag.get_utterance())\n",
    "            self.list_velocity_tensors.append(ag.velocity)\n",
    "            self.list_gaze_tensors.append(ag.velocity)\n",
    "            self.goal_location_tensors.append(ag.tensor_goal_location)\n",
    "            self.goal_type_tensors.append(ag.tensor_goal_type)\n",
    "            self.col_tensors.append(ag.col)\n",
    "            self.list_mem_tensors.append(ag.get_memory())\n",
    "            self.list_mem_last_tensors.append(ag.get_memory_last())\n",
    "            \n",
    "    def init_goals_tensors(self):\n",
    "        for ag in self.list_agents: \n",
    "            self.list_goals_tensors.append(ag.get_goal(self.list_agents))\n",
    "\n",
    "    def random_env_init(self):\n",
    "        self.name_of_targets = []\n",
    "        self.p = []\n",
    "        self.v = []\n",
    "        self.g = []\n",
    "        self.go = []\n",
    "        self.co = []\n",
    "        self.gl = []\n",
    "        self.utter = []\n",
    "        self.memory = []\n",
    "        self.memory_last = []\n",
    "        for i in range(self.nb_agents):\n",
    "            #self.name_of_targets.append(np.random.randint(0, self.nb_agents, (1, self.batch_size)))\n",
    "            self.name_of_targets.append((np.ones((1, self.batch_size))*i))\n",
    "            bound = 5\n",
    "            self.p.append(np.round(np.random.uniform(-bound, bound, [2, self.batch_size]), 2))\n",
    "            self.gl.append(np.round(np.random.uniform(-bound, bound, [2, self.batch_size]), 2))\n",
    "            self.v.append(np.zeros([self.env_dim, self.batch_size]))\n",
    "            self.g.append(np.zeros([self.env_dim, self.batch_size]))\n",
    "            J = np.random.choice(self.goal_type_size, self.batch_size)\n",
    "            self.go.append(np.transpose(self.enc.fit_transform(J.reshape(-1,1))))\n",
    "            #self.go.append(np.transpose(np.array([[1,0, 0] for i in range(self.batch_size)])))\n",
    "            self.co.append(np.random.uniform(0, 255, [3, 1]))\n",
    "            \n",
    "            self.utter.append(np.zeros([20, self.batch_size]))\n",
    "            self.memory.append(np.zeros([32, self.batch_size]))\n",
    "            self.memory_last.append(np.zeros([32, self.batch_size]))\n",
    "            \n",
    "            \n",
    "        \n",
    "        #self.init_agents()\n",
    "        #self.init_goals_tensors()   \n",
    "        return self.p, self.gl, self.v, self.g, self.go, self.utter, self.co, self.memory, self.memory_last, self.name_of_targets\n",
    "    \n",
    "    def get_placeholders(self):\n",
    "        return [self.list_agents, self.list_phys_tensors, self.list_utter_tensors,self.list_velocity_tensors, \n",
    "        self.list_gaze_tensors, self.goal_location_tensors, self.goal_type_tensors, self.col_tensors, \n",
    "        self.list_mem_tensors, self.list_mem_last_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, nb_agents, nb_landmarks, time_horizon, batch_size = 1024, time_delta = 0.1):\n",
    "        #tf.reset_default_graph()\n",
    "        self.time_horizon = time_horizon\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_agents = nb_agents\n",
    "        self.nb_landmarks = nb_landmarks\n",
    "        self.time_delta = time_delta\n",
    "        \n",
    "        self.instantiate_environment()\n",
    "        self.define_placeholders()\n",
    "        \n",
    "    def instantiate_environment(self):## ADD THE AUXILIARY REWARDS !!!!!\n",
    "        self.env = Environment(nb_agents = self.nb_agents)\n",
    "        self.policy = Policy(self.nb_agents, self.nb_landmarks, self.env.goal_size)\n",
    "     \n",
    "    def define_placeholders(self):\n",
    "        (self.h_pos, self.h_utter, self.h_velocity, self.h_gaze, self.h_goal_location, self.h_goal_type, \n",
    "         self.h_col, self.h_mem, self.h_last_mem, self.h_name_targets) = self.policy.get_placeholders()\n",
    "        self.list_goals = self.policy.get_list_goals(self.h_name_targets)\n",
    "        \n",
    "    def create_feed_dict(self, p, gl, v, g, go, utter, co, memory, memory_last, name_targets):\n",
    "        init_values = p + gl + v + g + go + utter + co + memory + memory_last + name_targets\n",
    "        placeholders_list = (self.h_pos + self.h_goal_location + self.h_velocity + self.h_gaze + self.h_goal_type + \n",
    "                             self.h_utter + self.h_col + self.h_mem + self.h_last_mem + self.h_name_targets)\n",
    "        feed_dict = {a:b for a,b in zip(placeholders_list, init_values)}\n",
    "        return feed_dict\n",
    "        \n",
    "    def train_batch(self, step_number, sess):\n",
    "        self.overall_res = []\n",
    "        self.all_env = []\n",
    "        t = tf.constant(1)\n",
    "        return_sofar = tf.constant(0.0)\n",
    "        self.suite_pos = []\n",
    "        arguments = [self.h_pos, self.h_gaze, self.h_utter, self.h_velocity, self.list_goals, self.h_mem, self.h_last_mem, \n",
    "                     self.h_goal_location, self.h_name_targets, self.h_goal_type, return_sofar, t]\n",
    "        result = tf.while_loop(self.condition, self.body, loop_vars = arguments)\n",
    "        #step = optimizer.minimize(-result[-2])\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        #starter_learning_rate = 0.00001 #0.97\n",
    "        learning_rate = 0.00001\n",
    "        #learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "        #                                   1, 0.8, staircase=True)\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "        tf.summary.scalar('accuracy', result[-2])\n",
    "        tf.summary.scalar('learning rate', learning_rate)\n",
    "        grads = optimizer.compute_gradients(-result[-2])\n",
    "        step = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "        for index, grad in enumerate(grads):\n",
    "            tf.summary.histogram(\"{}-grad\".format(grads[index][1].name), grads[index])\n",
    "        \n",
    "        print(\"Initializing variables\")\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        merged = tf.summary.merge_all()\n",
    "        file_writer = tf.summary.FileWriter('Summary/train',sess.graph)\n",
    "        sess.run(init_op)\n",
    "        sess.graph.finalize()\n",
    "        print(\"Start running\")\n",
    "        start = datetime.now()\n",
    "        for j in range(step_number):\n",
    "            p, gl, v, g, go, utter, co,  memory, memory_last, name_targets = self.env.random_env_init()\n",
    "            self.all_env.append([p,gl,v,g,go,utter,co,memory,memory_last,name_targets])\n",
    "            self.feed_dict = self.create_feed_dict(p, gl, v, g, go, utter, co, memory, memory_last, name_targets)\n",
    "            list_output = sess.run([step] + result, self.feed_dict)\n",
    "            #file_writer.add_summary(list_output[1], j)\n",
    "            self.overall_res.append(list_output)\n",
    "            if j%10 == 0:\n",
    "                final_pos = list_output[1]\n",
    "                final_gaze = list_output[2]\n",
    "                final_dist = [np.sqrt(np.sum((pos - gl[k])**2, axis = 0)) for k,pos in enumerate(final_pos)]\n",
    "                final_gaze = [np.sqrt(np.sum((gaze - gl[k])**2, axis = 0)) for k,gaze in enumerate(final_gaze)]\n",
    "                zeros = np.zeros(self.batch_size)\n",
    "                self.overall_res.append(list_output)\n",
    "                print(\"iteration \" + str(j))\n",
    "                print(list_output[-2])\n",
    "                for l in range(self.nb_agents):\n",
    "                    r = np.stack([final_dist[l], final_gaze[l], zeros], axis = 0)\n",
    "                    dist_obj = np.sum(np.multiply(r, go[l]), axis = 0)\n",
    "                    final_dist_mean = np.mean(dist_obj)\n",
    "                    final_dist_median = np.median(dist_obj)\n",
    "                    print(\"-- Agent\" + str(l))\n",
    "                    print(\"---final distance mean: \" + str(final_dist_mean))\n",
    "                    print(\"---final distance median:\" + str(final_dist_mean))\n",
    "                    \n",
    "                print(\"computing time\")\n",
    "                print(datetime.now() - start)\n",
    "                start = datetime.now()\n",
    "                print(\"\\n\")\n",
    "        \n",
    "        return self.overall_res\n",
    "    \n",
    "    # Structure of args [h_pos, h_utterance, h_velocity, list_goals, h_mem, h_mem_last, return_sofar, t]\n",
    "    def body(self, h_pos, h_gaze, h_utterance, h_velocity, list_goals, h_mem, h_mem_last, h_goal_locations, h_name_targets, \n",
    "             h_goal_type, return_sofar, t):\n",
    "        list_moves, list_gazes, list_utterances, list_delta_mem, list_last_delta_mem =self.policy.compute_output(\n",
    "                h_pos, h_utterance, list_goals, h_mem, h_mem_last)\n",
    "        new_pos_agents = []\n",
    "        new_utterance_agents = []\n",
    "        new_velocity_agents = []\n",
    "        new_gaze_agents = []\n",
    "        new_h_mem_agents = []\n",
    "        new_h_mem_last_agents = []\n",
    "        for i in range(self.env.nb_agents):\n",
    "            output_utterance = list_utterances[i]\n",
    "            output_velocity = list_moves[i]\n",
    "            output_gaze = list_gazes[i]\n",
    "            memory_delta = list_delta_mem[i]\n",
    "            memory_delta_last = list_last_delta_mem[i]\n",
    "            \n",
    "            new_pos, new_velocity, new_gaze, new_utterance, new_memory, new_memory_last = self.policy.compute_new_state(\n",
    "                output_utterance, output_velocity, output_gaze, memory_delta, memory_delta_last, h_pos[i], \n",
    "                h_velocity[i], h_mem[i], h_mem_last[i])\n",
    "            \n",
    "            new_pos_agents.append(new_pos)\n",
    "            new_velocity_agents.append(new_velocity)\n",
    "            new_utterance_agents.append(new_utterance)\n",
    "            new_gaze_agents.append(new_gaze)\n",
    "            new_h_mem_agents.append(new_memory)\n",
    "            new_h_mem_last_agents.append(new_memory_last)\n",
    "\n",
    "        new_reward_batch = self.policy.compute_reward_agents(new_pos_agents, new_gaze_agents, new_velocity_agents,\n",
    "                                              new_utterance_agents, h_goal_locations, h_name_targets, h_goal_type)\n",
    "        \n",
    "\n",
    "        return_sofar += new_reward_batch\n",
    "        t += 1\n",
    "        \n",
    "        return [new_pos_agents, new_gaze_agents, new_utterance_agents, new_velocity_agents, list_goals, new_h_mem_agents, new_h_mem_last_agents, \n",
    "        h_goal_locations, h_name_targets, h_goal_type, return_sofar, t]\n",
    "        \n",
    "    # Structure of args [h_pos, h_utterance, h_velocity, list_goals, h_mem, h_mem_last, return_sofar, t]\n",
    "    def condition(self, h_pos, h_gaze, h_utterance, h_velocity, list_goals, h_mem, h_mem_last, h_goal_locations, \n",
    "                  h_name_targets, h_goal_type, return_sofar, t):\n",
    "        return tf.less(t, self.time_horizon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"while/Reshape_8:0\", shape=(1024, 1), dtype=int32)\n",
      "Tensor(\"while/Reshape_14:0\", shape=(1024, 1), dtype=int32)\n",
      "INFO:tensorflow:Summary name learning rate is illegal; using learning_rate instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name learning rate is illegal; using learning_rate instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/weight_0:0-grad is illegal; using phys_variable/weight_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/weight_0:0-grad is illegal; using phys_variable/weight_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/weight_1:0-grad is illegal; using phys_variable/weight_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/weight_1:0-grad is illegal; using phys_variable/weight_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/weight_2:0-grad is illegal; using phys_variable/weight_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/weight_2:0-grad is illegal; using phys_variable/weight_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/bias_0:0-grad is illegal; using phys_variable/bias_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/bias_0:0-grad is illegal; using phys_variable/bias_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/bias_1:0-grad is illegal; using phys_variable/bias_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/bias_1:0-grad is illegal; using phys_variable/bias_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/bias_2:0-grad is illegal; using phys_variable/bias_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name phys_variable/bias_2:0-grad is illegal; using phys_variable/bias_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Variable:0-grad is illegal; using Variable_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Variable:0-grad is illegal; using Variable_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable/com_net_weight_0:0-grad is illegal; using com_variable/com_net_weight_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable/com_net_weight_0:0-grad is illegal; using com_variable/com_net_weight_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable/com_net_weight_1:0-grad is illegal; using com_variable/com_net_weight_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable/com_net_weight_1:0-grad is illegal; using com_variable/com_net_weight_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable/com_net_weight_2:0-grad is illegal; using com_variable/com_net_weight_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable/com_net_weight_2:0-grad is illegal; using com_variable/com_net_weight_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable_1/com_net_bias_0:0-grad is illegal; using com_variable_1/com_net_bias_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable_1/com_net_bias_0:0-grad is illegal; using com_variable_1/com_net_bias_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable_1/com_net_bias_1:0-grad is illegal; using com_variable_1/com_net_bias_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable_1/com_net_bias_1:0-grad is illegal; using com_variable_1/com_net_bias_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable_1/com_net_bias_2:0-grad is illegal; using com_variable_1/com_net_bias_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name com_variable_1/com_net_bias_2:0-grad is illegal; using com_variable_1/com_net_bias_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name weight_mem_com:0-grad is illegal; using weight_mem_com_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name weight_mem_com:0-grad is illegal; using weight_mem_com_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name bias_mem_com:0-grad is illegal; using bias_mem_com_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name bias_mem_com:0-grad is illegal; using bias_mem_com_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Variable_1:0-grad is illegal; using Variable_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Variable_1:0-grad is illegal; using Variable_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable/lastnet_weight_0:0-grad is illegal; using last_variable/lastnet_weight_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable/lastnet_weight_0:0-grad is illegal; using last_variable/lastnet_weight_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable/last_net_weight_1:0-grad is illegal; using last_variable/last_net_weight_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable/last_net_weight_1:0-grad is illegal; using last_variable/last_net_weight_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable/last_net_weight_2:0-grad is illegal; using last_variable/last_net_weight_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable/last_net_weight_2:0-grad is illegal; using last_variable/last_net_weight_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable_1/last_net_bias_0:0-grad is illegal; using last_variable_1/last_net_bias_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable_1/last_net_bias_0:0-grad is illegal; using last_variable_1/last_net_bias_0_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable_1/last_net_bias_1:0-grad is illegal; using last_variable_1/last_net_bias_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable_1/last_net_bias_1:0-grad is illegal; using last_variable_1/last_net_bias_1_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable_1/last_net_bias_2:0-grad is illegal; using last_variable_1/last_net_bias_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name last_variable_1/last_net_bias_2:0-grad is illegal; using last_variable_1/last_net_bias_2_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name weight_mem_last:0-grad is illegal; using weight_mem_last_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name weight_mem_last:0-grad is illegal; using weight_mem_last_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name bias_mem_last:0-grad is illegal; using bias_mem_last_0-grad instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name bias_mem_last:0-grad is illegal; using bias_mem_last_0-grad instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing variables\n",
      "Start running\n",
      "iteration 0\n",
      "-1789.47\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-3e8bc9d54ee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-e8a5e99763d4>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, step_number, sess)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iteration \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_agents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfinal_dist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_gaze\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mdist_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48.4 s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "exp = Experiment(2, 0, 50)\n",
    "with tf.Session() as sess:\n",
    "    l = exp.train_batch(5000, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 921 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "r = [exp.overall_res[i][-2] for i in range(len(exp.overall_res))]\n",
    "dr = pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAFkCAYAAADoo9t2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4dc704b5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.79 s\n"
     ]
    }
   ],
   "source": [
    "#pd.rolling_mean(dr, 60).plot()\n",
    "plt.plot(r)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-182937.19,\n",
       " -182937.19,\n",
       " -58116.5,\n",
       " -15716.643,\n",
       " -74588.805,\n",
       " -60870.434,\n",
       " -94771.922,\n",
       " -58028.039,\n",
       " -31981.943,\n",
       " -30729.436,\n",
       " -27055.594,\n",
       " -46303.738,\n",
       " -46303.738,\n",
       " -27625.006,\n",
       " -9790.7119,\n",
       " -68469.773,\n",
       " -16773.783,\n",
       " -23215.176,\n",
       " -10358.177,\n",
       " -47120.938,\n",
       " -10080.265,\n",
       " -13030.263,\n",
       " -22157.701,\n",
       " -22157.701,\n",
       " -18941.359,\n",
       " -13098.636,\n",
       " -9756.0957,\n",
       " -13227.911,\n",
       " -19204.219,\n",
       " -9579.959,\n",
       " -19424.828,\n",
       " -24096.383,\n",
       " -11794.164,\n",
       " -9276.3633,\n",
       " -9276.3633,\n",
       " -15492.31,\n",
       " -13280.396,\n",
       " -9519.8477,\n",
       " -9189.4082,\n",
       " -11323.009,\n",
       " -9812.6641,\n",
       " -30201.002,\n",
       " -10629.354,\n",
       " -11715.265,\n",
       " -9973.3242,\n",
       " -9973.3242,\n",
       " -10271.856,\n",
       " -11241.281,\n",
       " -11192.56,\n",
       " -10017.45,\n",
       " -12653.22,\n",
       " -9235.8359,\n",
       " -10840.249,\n",
       " -19158.432,\n",
       " -9851.3223,\n",
       " -9631.9766,\n",
       " -9631.9766,\n",
       " -9319.8818,\n",
       " -10204.132,\n",
       " -11105.599,\n",
       " -9811.1533,\n",
       " -15095.022,\n",
       " -16264.184,\n",
       " -9364.1768,\n",
       " -22028.66,\n",
       " -11208.357,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.4 ms\n"
     ]
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4]\n",
      " [2 1]]\n",
      "time: 543 ms\n"
     ]
    }
   ],
   "source": [
    "e = tf.constant([[1, 2], [3, 4]])\n",
    "g = tf.gather_nd(e, [tf.constant([[1, 0], [1, 1]]), tf.constant([[0, 1], [0, 0]])])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.38 ms\n"
     ]
    }
   ],
   "source": [
    "e = tf.constant([[0, 1, 2], [1, 0, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_16:0' shape=(2, 3) dtype=int32>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.49 ms\n"
     ]
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 42.1 ms\n"
     ]
    }
   ],
   "source": [
    "np.median([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-87063.805,\n",
       " -87063.805,\n",
       " -42613.492,\n",
       " -95466.906,\n",
       " -51702.867,\n",
       " -119452.19,\n",
       " -54361.605,\n",
       " -45506.168,\n",
       " -45355.227,\n",
       " -54363.707,\n",
       " -50592.652,\n",
       " -64282.707,\n",
       " -64282.707,\n",
       " -44239.941,\n",
       " -42706.688,\n",
       " -48918.352,\n",
       " -31315.318,\n",
       " -12464.795,\n",
       " -16229.566,\n",
       " -17335.041,\n",
       " -34374.93,\n",
       " -33092.418,\n",
       " -33758.09,\n",
       " -33758.09,\n",
       " -23054.051,\n",
       " -26396.902,\n",
       " -23441.33,\n",
       " -13553.037,\n",
       " -16636.049,\n",
       " -22000.59,\n",
       " -15910.657,\n",
       " -17852.219,\n",
       " -22044.727,\n",
       " -16584.189,\n",
       " -16584.189,\n",
       " -13721.631,\n",
       " -17386.037,\n",
       " -17495.488,\n",
       " -17994.559,\n",
       " -15598.205,\n",
       " -9668.4219,\n",
       " -18942.807,\n",
       " -17743.613,\n",
       " -9740.2227,\n",
       " -14156.621,\n",
       " -14156.621,\n",
       " -10293.151,\n",
       " -12772.105,\n",
       " -14022.875,\n",
       " -12508.376,\n",
       " -13253.797,\n",
       " -13280.086,\n",
       " -9958.9531,\n",
       " -6889.3926,\n",
       " -9030.5986,\n",
       " -14340.563,\n",
       " -14340.563,\n",
       " -11904.121,\n",
       " -10879.037,\n",
       " -7775.1309,\n",
       " -12467.575,\n",
       " -12387.378,\n",
       " -11923.172,\n",
       " -11323.783,\n",
       " -11144.868,\n",
       " -10955.39,\n",
       " -8239.79,\n",
       " -8239.79,\n",
       " -8856.7529,\n",
       " -8973.5801,\n",
       " -10009.217,\n",
       " -8228.1475,\n",
       " -9959.7656,\n",
       " -8921.8408,\n",
       " -9621.7822,\n",
       " -8656.2256,\n",
       " -8147.7202,\n",
       " -9009.5645,\n",
       " -9009.5645,\n",
       " -8581.2041,\n",
       " -9847.1289,\n",
       " -8783.6055,\n",
       " -8135.1001,\n",
       " -8027.188,\n",
       " -8681.8428,\n",
       " -7703.0547,\n",
       " -10157.399,\n",
       " -8117.313,\n",
       " -8001.7373,\n",
       " -8001.7373,\n",
       " -8407.6816,\n",
       " -7545.3403,\n",
       " -8257.0498,\n",
       " -8077.3325,\n",
       " -9683.75,\n",
       " -8234.916,\n",
       " -8465.2119,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.5 ms\n"
     ]
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-a268cdbdf94a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverall_res\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-206-a268cdbdf94a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverall_res\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gl' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.4 ms\n"
     ]
    }
   ],
   "source": [
    "d = [np.mean(np.sqrt(np.sum((pos[1] - gl[0])**2, axis = 0))) for pos in exp.overall_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  1.,  1.,  1.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.51 ms\n"
     ]
    }
   ],
   "source": [
    "n_samples = 20\n",
    "n_classes = 3\n",
    "J = np.random.choice(n_classes, n_samples)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(n_values=n_classes, sparse=False)\n",
    "np.transpose(enc.fit_transform(J.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "time: 2.03 ms\n"
     ]
    }
   ],
   "source": [
    "for i, k in enumerate(zip([1, 2, 3], [4, 5, 6], [7, 8, 9])):\n",
    "    print(np.dot(k, [0, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  2.,  2.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.02 ms\n"
     ]
    }
   ],
   "source": [
    "np.ones((1, 3))*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
