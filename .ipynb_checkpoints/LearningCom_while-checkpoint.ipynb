{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from datetime import datetime \n",
    "%load_ext autotime\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method LineWatcher.stop of <autotime.LineWatcher object at 0x000000CEC04B2BA8>> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\autotime.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[1;32massert\u001b[0m \u001b[0mdiff\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mformat_delta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.02 ms\n"
     ]
    }
   ],
   "source": [
    "# Param: x, stacking of the output of fully connected physical network for each agent. Shape = (256, batch_size, nb_agents)\n",
    "# return: pooling of input features.\n",
    "def softmax_pooling(x):\n",
    "    # pooling function. Softmax pooling is compromise between max pooling and average pooling\n",
    "    coefs = tf.nn.softmax(x, dim = -1)\n",
    "    softmax_pool = tf.reduce_sum(tf.multiply(coefs, x), axis = -1)\n",
    "    return softmax_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 44 ms\n"
     ]
    }
   ],
   "source": [
    "class PhysicalNet: \n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, output_size, keep_prob = 0.9, stddev = 0.0001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.init_biases()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        with tf.variable_scope(\"phys_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                if i == 0:\n",
    "                    W = tf.get_variable(\"weight_\" + str(i), shape=[self.layer_sizes[i], self.input_size],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    tf.summary.histogram('phys_net_weight_'+ str(i), W)\n",
    "                elif i != (self.nb_layers - 1):\n",
    "                    W = tf.get_variable(\"weight_\" + str(i), shape=[self.layer_sizes[(i+1)], self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    tf.summary.histogram('phys_net_weight_'+ str(i), W)\n",
    "                else:\n",
    "                    W = tf.get_variable(\"weight_\" + str(i), shape=[self.output_size, self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    tf.summary.histogram('phys_net_weight_'+ str(i), W)\n",
    "\n",
    "                self.Weights.append(W)\n",
    "            \n",
    "    def init_biases(self):\n",
    "        with tf.variable_scope(\"phys_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                B = tf.get_variable(\"bias_\" + str(i), shape=[self.layer_sizes[i], 1],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                tf.summary.histogram('phys_net_bias_'+ str(i), B)\n",
    "                self.Biases.append(B)\n",
    "            \n",
    "            \n",
    "    def compute_output(self, x): \n",
    "            for i in range(self.nb_layers):\n",
    "                W = self.Weights[i]\n",
    "                b = self.Biases[i]\n",
    "                if i != (self.nb_layers - 1):\n",
    "                    x = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "                else:\n",
    "                    x = tf.nn.elu(tf.matmul(W, x) + b)\n",
    "            \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 72 ms\n"
     ]
    }
   ],
   "source": [
    "class CommunicationNet: \n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, keep_prob = 0.9, memory_size = 32,\n",
    "                 stddev_epsilon = 0.35, output_size = 256, stddev = 0.0001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.memory_size = memory_size\n",
    "        self.stddev_epsilon = stddev_epsilon\n",
    "        self.output_size = output_size\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        self.Weight_read_mem = tf.Variable(tf.random_normal([self.layer_sizes[(self.nb_layers-1)], self.memory_size]\n",
    "                                                            ,stddev = self.stddev))\n",
    "        \n",
    "        self.init_weights()    \n",
    "        self.init_biases()\n",
    "        self.def_delta_mem()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        with tf.variable_scope(\"com_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                if i == 0:\n",
    "                    W = tf.get_variable(\"com_net_weight_\" + str(i), shape=[self.layer_sizes[i], self.input_size],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    tf.summary.histogram('com_net_weight_'+ str(i), W)\n",
    "                elif i != (self.nb_layers - 1):\n",
    "                    W = tf.get_variable(\"com_net_weight_\" + str(i), shape=[self.layer_sizes[(i+1)], self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    tf.summary.histogram('com_net_weight_'+ str(i), W)\n",
    "                else:\n",
    "                    W = tf.get_variable(\"com_net_weight_\" + str(i), shape=[self.output_size, self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "\n",
    "                    tf.summary.histogram('com_net_weight_'+ str(i), W)\n",
    "\n",
    "                self.Weights.append(W)\n",
    "       \n",
    "    \n",
    "    def init_biases(self):\n",
    "        with tf.variable_scope(\"com_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                #B = tf.Variable(tf.random_normal([self.layer_sizes[i], 1],stddev = self.stddev), name = \"bias_\" + str(i))\n",
    "                B = tf.Variable(tf.zeros([self.layer_sizes[i], 1]), name = \"com_net_bias_\" + str(i))\n",
    "                tf.summary.histogram('com_net_bias_'+ str(i), B)\n",
    "                self.Biases.append(B)\n",
    "       \n",
    "    \n",
    "    def def_delta_mem(self):\n",
    "        self.W_mem = tf.get_variable(\"weight_mem_com\" , shape=[self.memory_size,self.output_size],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "        self.b_mem = tf.Variable(tf.zeros(shape = [self.memory_size, 1]), name = \"bias_mem_com\")\n",
    "\n",
    "        \n",
    "    def compute_output(self, x, memory):\n",
    "        for i in range(self.nb_layers):\n",
    "            W = self.Weights[i]\n",
    "            b = self.Biases[i]\n",
    "            if i != (self.nb_layers - 1):\n",
    "                x = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "            else:\n",
    "                x = tf.nn.elu(tf.matmul(W, x) + tf.matmul(self.Weight_read_mem, memory) + b)\n",
    "                \n",
    "            \n",
    "        delta_mem = tf.add(tf.matmul(self.W_mem, x),self.b_mem)\n",
    "        return x, delta_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 84 ms\n"
     ]
    }
   ],
   "source": [
    "class LastNet: \n",
    "    \n",
    "    def __init__(self, layer_sizes, input_size, keep_prob = 0.9, memory_size = 32, \n",
    "                 stddev_epsilon = 0.35, output_size = 24, stddev = 0.0001):\n",
    "        self.nb_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.input_size = input_size\n",
    "        self.memory_size = memory_size\n",
    "        self.stddev_epsilon = stddev_epsilon\n",
    "        self.output_size = output_size\n",
    "        self.stddev = stddev\n",
    "        \n",
    "        self.Weights = []\n",
    "        self.Biases = []\n",
    "        self.Weight_read_mem = tf.Variable(tf.random_normal([self.output_size, self.memory_size],stddev = self.stddev))\n",
    "        \n",
    "        self.init_weights()    \n",
    "        self.init_biases()\n",
    "        self.def_delta_mem()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        with tf.variable_scope(\"last_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                if i == 0:\n",
    "                    W = tf.get_variable(\"lastnet_weight_\" + str(i), shape=[self.layer_sizes[i], self.input_size],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    tf.summary.histogram('last_net_weight_'+ str(i), W)\n",
    "                elif i != (self.nb_layers - 1):\n",
    "                    W = tf.get_variable(\"last_net_weight_\" + str(i), shape=[self.layer_sizes[(i+1)], self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    tf.summary.histogram('last_net_weight_'+ str(i), W)\n",
    "                else:\n",
    "                    W = tf.get_variable(\"last_net_weight_\" + str(i), shape=[self.output_size, self.layer_sizes[i]],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "                    tf.summary.histogram('last_net_weight_'+ str(i), W)\n",
    "\n",
    "                self.Weights.append(W)\n",
    "            \n",
    "    def init_biases(self):\n",
    "        with tf.variable_scope(\"last_variable\") as scope:\n",
    "            for i in range(self.nb_layers):\n",
    "                if i != (self.nb_layers - 1):\n",
    "                    B = tf.Variable(tf.zeros([self.layer_sizes[i], 1]), name = \"last_net_bias_\" + str(i))\n",
    "                    tf.summary.histogram('last_net_bias_'+ str(i), B)\n",
    "                else:\n",
    "                    B = tf.Variable(tf.zeros([self.output_size, 1]), name = \"last_net_bias_\" + str(i))\n",
    "                    tf.summary.histogram('last_net_bias_'+ str(i), B)\n",
    "\n",
    "                self.Biases.append(B)\n",
    "\n",
    "        \n",
    "    def def_delta_mem(self):\n",
    "        #self.W_mem = tf.Variable(tf.random_normal(shape =[self.memory_size,self.output_size],stddev = self.stddev),\n",
    "        #                        name = \"weight_mem_last\")\n",
    "        self.W_mem = tf.get_variable(\"weight_mem_last\", shape=[self.memory_size,self.output_size],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer(False))\n",
    "        self.b_mem = tf.Variable(tf.zeros(shape = [self.memory_size, 1]),\n",
    "                                name = \"bias_mem_last\")\n",
    "        \n",
    "    def compute_output(self, x, memory):## ADD THE DROPOUTS !!! REMOVE THE SOFTMAX OF THE LAST LAYER !!!\n",
    "        for i in range(self.nb_layers):\n",
    "            W = self.Weights[i]\n",
    "            b = self.Biases[i]\n",
    "            if i != (self.nb_layers - 1):\n",
    "                x = tf.nn.dropout(tf.nn.elu(tf.matmul(W, x) + b), keep_prob = 0.9)\n",
    "            else:\n",
    "                x = tf.nn.elu(tf.matmul(W, x) + tf.matmul(self.Weight_read_mem, memory) + b)\n",
    "               \n",
    "        delta_mem = tf.add(tf.matmul(self.W_mem, x),self.b_mem)\n",
    "        return x, delta_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20 ms\n"
     ]
    }
   ],
   "source": [
    "class Policy_Phys:\n",
    "    \n",
    "    def __init__(self, nb_agent, nb_landmark, hidden_layer_size = 256, env_dim = 2, \n",
    "                 batch_size = 1024, stddev_phys_output = 0.0001):\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.batch_size = batch_size\n",
    "        self.env_dim = env_dim\n",
    "        self.nb_agent = nb_agent\n",
    "        self.nb_landmark = nb_landmark\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.init_all()\n",
    "        \n",
    "    def init_phys_module(self):\n",
    "        self.network_phys = PhysicalNet([self.hidden_layer_size, self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                              self.env_dim, self.hidden_layer_size)\n",
    "\n",
    "\n",
    "    def compute_output(self, x_agent):\n",
    "        #Input: list of positions of agents. Output: softmax pooling of the output of FCx module for each agent.\n",
    "        output = []\n",
    "        for x in x_agent:\n",
    "            output.append(tf.reshape(self.network_phys.compute_output(x), [256, -1, 1]))\n",
    "            \n",
    "        all_phys_output = tf.concat(output, axis = 2)\n",
    "        self.PhiX = softmax_pooling(all_phys_output)\n",
    "        return self.PhiX\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_phys_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "class Policy_Utterance:\n",
    "    \n",
    "    def __init__(self, nb_agent, goal_size, vocabulary_size = 20, \n",
    "                 hidden_layer_size = 256, memory_size = 32, temperature = 1, batch_size = 1024,\n",
    "                 stddev_phys_output = 0.0001):\n",
    "        self.size_goal = goal_size\n",
    "        self.nb_agent = nb_agent\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.memory_size = memory_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.goal = tf.placeholder(tf.float32, [self.size_goal, None])\n",
    "        \n",
    "        self.init_all()\n",
    "        \n",
    "\n",
    "    def init_com_module(self):\n",
    "        self.network_com = CommunicationNet([self.hidden_layer_size, self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                             self.vocabulary_size)\n",
    "\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_com_module()\n",
    "\n",
    "    def compute_output(self, c_agent, mem_agent):\n",
    "        #Input: list of agent's utterance and memory. Output: pooling of output of com module for each agent and list of\n",
    "        # agent's communication memory.\n",
    "        output = []\n",
    "        delta_mem = []\n",
    "        for c, mem in zip(c_agent, mem_agent):\n",
    "            o, m = self.network_com.compute_output(c, mem)\n",
    "            output.append(tf.reshape(o, [256, -1, 1]))\n",
    "            delta_mem.append(m)\n",
    "\n",
    "        all_comm_output = tf.concat(output, axis = 2)\n",
    "        PhiC = softmax_pooling(all_comm_output)\n",
    "        \n",
    "        return PhiC, delta_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48 ms\n"
     ]
    }
   ],
   "source": [
    "class Policy_Last:\n",
    "    \n",
    "    def __init__(self, hidden_layer_size = 256, size_goal = 8, memory_size = 32, batch_size = 1024, \n",
    "                 stddev_phys_output = 0.0001, vocabulary_size = 20, env_dim = 2, temperature = 1):\n",
    "        self.temperature = temperature\n",
    "        self.stddev_phys_output = stddev_phys_output\n",
    "        self.env_dim = env_dim\n",
    "        self.vocabulary_size = vocabulary_size \n",
    "        self.batch_size = batch_size\n",
    "        self.memory_size = memory_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.size_goal = size_goal\n",
    "        self.init_all()\n",
    "\n",
    "    def init_last_module(self):\n",
    "        inp_size = (2*self.hidden_layer_size + self.size_goal)\n",
    "        out_size = self.vocabulary_size + 2*self.env_dim\n",
    "        self.last_net = LastNet([self.hidden_layer_size, self.hidden_layer_size, self.hidden_layer_size], \n",
    "                                inp_size, output_size = out_size)\n",
    "\n",
    "    def init_all(self):\n",
    "        self.init_last_module()\n",
    "\n",
    "    def sample_utterance(self, output):\n",
    "        # Application of gumbel-softmax trick\n",
    "        # Input: output of preceding networks. \n",
    "        u = -tf.log(-tf.log(tf.random_uniform(shape = [self.vocabulary_size, self.batch_size],dtype=tf.float32)))\n",
    "        utterance_output = tf.slice(output, [2*self.env_dim, 0], [self.vocabulary_size, self.batch_size])\n",
    "        gumbel = tf.exp((utterance_output + u)/self.temperature)\n",
    "        denoms = tf.reduce_sum(gumbel, axis = 0)\n",
    "        utterance = gumbel/denoms\n",
    "        return utterance\n",
    "        \n",
    "    def sample_phys(self, output):\n",
    "        #Input output of the last network.\n",
    "        #Output: sampled values for new velocity and gaze\n",
    "        u = tf.random_normal(shape = [2*self.env_dim, self.batch_size],dtype=tf.float32, stddev = self.stddev_phys_output)\n",
    "        o = tf.add(tf.slice(output, [0, 0], [2*self.env_dim, self.batch_size]), u)\n",
    "        sample_move = tf.slice(o, [0, 0], [self.env_dim, self.batch_size])\n",
    "        sample_gaze  = tf.slice(o, [self.env_dim, 0], [self.env_dim, self.batch_size])\n",
    "        return sample_move, sample_gaze\n",
    "            \n",
    "    def compute_output(self, PhiX, PhiC, last_mem, goal):\n",
    "        #Input: outputs of physical and communication modules and goals.\n",
    "        #Output: move, gaze, utterance and memory.\n",
    "        Phi = tf.concat([PhiX, goal, PhiC], axis = 0)\n",
    "        output, memory = self.last_net.compute_output(Phi, last_mem)\n",
    "        utterance = self.sample_utterance(output)\n",
    "        move, gaze = self.sample_phys(output)\n",
    "        return move, gaze, utterance, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 380 ms\n"
     ]
    }
   ],
   "source": [
    "class Policy:\n",
    "\n",
    "    def __init__(self,nb_agent, nb_landmark, goal_size, vocabulary_size = 20, hidden_layer_size = 256,\n",
    "                 memory_size = 32, temperature = 1, batch_size = 1024, stddev_phys_output = 0.0001, env_dim = 2,\n",
    "                 goal_type_size = 3, time_delta = 0.1, damping_coeff = 0.5):\n",
    "        self.nb_agent = nb_agent\n",
    "        self.goal_size = goal_size\n",
    "        self.nb_landmark = nb_landmark\n",
    "        self.env_dim = env_dim\n",
    "        self.goal_type_size = goal_type_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.memory_size = memory_size\n",
    "        self.time_delta = time_delta\n",
    "        self.damping_coeff = damping_coeff\n",
    "     \n",
    "        self.phys_module = Policy_Phys(self.nb_agent, self.nb_landmark)\n",
    "        self.utterance_module = Policy_Utterance(self.nb_agent, self.goal_size)\n",
    "        self.last_module = Policy_Last()\n",
    "        \n",
    "        self.list_PhiX = []\n",
    "        self.list_PhiC = []\n",
    "        self.init_placeholders()\n",
    "         \n",
    "    def init_placeholders(self):\n",
    "        self.h_pos = [tf.placeholder(tf.float32, shape = [self.env_dim, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_utterance = [tf.placeholder(tf.float32, shape = [self.vocabulary_size, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_velocity = [tf.placeholder(tf.float32, shape = [self.env_dim, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_gaze = [tf.placeholder(tf.float32, shape = [self.env_dim, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_goal_location = [tf.placeholder(tf.float32, shape = [self.env_dim, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_goal_type = [tf.placeholder(tf.float32, shape = [self.goal_type_size, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_col = [tf.placeholder(tf.float32, shape = [3, 1]) for i in range(self.nb_agent)]\n",
    "        self.h_mem = [tf.placeholder(tf.float32, shape = [self.memory_size, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_mem_last = [tf.placeholder(tf.float32, shape = [self.memory_size, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        self.h_name_targets = [tf.placeholder(tf.int32, shape = [1, self.batch_size]) for i in range(self.nb_agent)]\n",
    "        \n",
    "        \n",
    "    def get_placeholders(self):\n",
    "        return [self.h_pos, self.h_utterance, self.h_velocity, self.h_gaze, self.h_goal_location, self.h_goal_type,\n",
    "        self.h_col, self.h_mem, self.h_mem_last, self.h_name_targets]\n",
    "    \n",
    "    def get_list_goals(self, list_name_targets):\n",
    "        list_goals = []\n",
    "        for i in range(self.nb_agent):\n",
    "            list_goals.append(self.init_goal_agent(i))\n",
    "            \n",
    "        return list_goals\n",
    "\n",
    "    def init_goal_agent(self, agent_number):\n",
    "        other_agents = tf.gather(self.h_col, self.h_name_targets[agent_number])\n",
    "        colors = tf.reshape(tf.concat(other_agents, axis = 1), [3, self.batch_size])\n",
    "        return tf.concat([self.h_goal_type[agent_number], self.h_goal_location[agent_number], colors], axis = 0)\n",
    "        \n",
    "    def compute_output(self, list_pos, list_utterances, list_goals, list_mem, list_last_mem):\n",
    "        list_moves = []\n",
    "        list_gazes = []\n",
    "        list_new_utterances = []\n",
    "        list_last_delta_mem = []\n",
    "        PhiX = self.phys_module.compute_output(list_pos)\n",
    "        PhiC, list_delta_mem = self.utterance_module.compute_output(list_utterances, list_mem)\n",
    "        self.list_PhiX.append(PhiX)\n",
    "        self.list_PhiC.append(PhiC)\n",
    "        for last_mem, goal in zip(list_last_mem, list_goals):\n",
    "            m, g, u, ldm = self.last_module.compute_output(PhiX, PhiC, last_mem, goal)\n",
    "            list_moves.append(m)\n",
    "            list_gazes.append(g)\n",
    "            list_new_utterances.append(u)\n",
    "            list_last_delta_mem.append(ldm)\n",
    "            \n",
    "        return list_moves, list_gazes, list_new_utterances, list_delta_mem, list_last_delta_mem\n",
    "    \n",
    "    def compute_reward_agents(self, new_pos, new_gaze, list_move, new_utterance, goal_locations, list_targets, goal_type): \n",
    "        rewards = []\n",
    "        ag_positions = []\n",
    "        ag_gazes = []\n",
    "        ag_goal_on_agent = []\n",
    "        ag_velocities = []\n",
    "        ag_utterances = []\n",
    "        ag_goal_locations = []\n",
    "        ag_goal_type = []\n",
    "        for i in range(self.nb_agent):## Useless, take list directly\n",
    "            ag_positions.append(new_pos[i])\n",
    "            ag_gazes.append(new_gaze[i])\n",
    "            ag_velocities.append(list_move[i])\n",
    "            ag_utterances.append(new_utterance[i])\n",
    "            ag_goal_locations.append(goal_locations[i])\n",
    "            ag_goal_type.append(goal_type[i])\n",
    "            \n",
    "        agent_positions = tf.stack(ag_positions, axis = 2)\n",
    "        agent_gazes = tf.stack(ag_gazes, axis = 2)\n",
    "        agent_velocities = tf.stack(ag_velocities, axis = 2)\n",
    "        agent_utterances = tf.stack(ag_utterances, axis = 2)\n",
    "        agent_goal_locations = tf.stack(ag_goal_locations, axis = 2)\n",
    "        agent_goal_type = tf.stack(ag_goal_type, axis = 2)\n",
    "        \n",
    "        for i in range(self.nb_agent):\n",
    "            name_target = tf.reshape(tf.gather_nd(list_targets[i], [[0,k] for k in range(self.batch_size)]), [self.batch_size, 1])\n",
    "            l1 = tf.concat([tf.constant([[0] for i in range(self.batch_size)]), tf.constant([[i] for i in range(self.batch_size)]),\n",
    "                           name_target], axis = 1) \n",
    "            l2 = tf.concat([tf.constant([[1] for i in range(self.batch_size)]), tf.constant([[i] for i in range(self.batch_size)]),\n",
    "                           name_target], axis = 1)            \n",
    "            l3 = [l1, l2]\n",
    "\n",
    "            position_target = tf.gather_nd(agent_positions, l3)\n",
    "            gaze_target = tf.gather_nd(agent_gazes, l3)\n",
    "            \n",
    "            own_velocity = tf.reshape(tf.slice(agent_velocities, [0, 0, i], [self.env_dim, self.batch_size, 1]), \n",
    "                                         [self.env_dim, self.batch_size])  \n",
    "            \n",
    "            own_gaze = tf.reshape(tf.slice(agent_gazes, [0, 0, i], [self.env_dim, self.batch_size, 1]), \n",
    "                                         [self.env_dim, self.batch_size])\n",
    "            \n",
    "            own_goal_location = tf.reshape(tf.slice(agent_goal_locations, [0, 0, i], [self.env_dim, self.batch_size, 1]), \n",
    "                                          [self.env_dim, self.batch_size])\n",
    "            \n",
    "            own_utterance = tf.reshape(tf.slice(agent_utterances, [0, 0, i], [self.vocabulary_size, self.batch_size, 1]), \n",
    "                                         [self.vocabulary_size, self.batch_size])\n",
    "            \n",
    "            own_goal_type = tf.reshape(tf.slice(agent_goal_type, [0, 0, i], [3, self.batch_size, 1]), \n",
    "                                          [3, self.batch_size])\n",
    "            \n",
    "            reward_agent = self.compute_reward_agent(position_target, gaze_target, own_velocity, own_gaze,\n",
    "                                                      own_utterance, own_goal_location, own_goal_type)\n",
    "            rewards.append(reward_agent)\n",
    "          \n",
    "        #check axis = 0 is okay\n",
    "        rewards_batch = tf.reduce_mean(tf.reduce_sum(tf.stack(rewards, axis = 1), axis = 1), axis = 0)\n",
    "        return rewards_batch\n",
    "    \n",
    "    \n",
    "    def compute_reward_agent(self,goal_agent_pos, goal_agent_gaze, output_velocity, output_gaze, new_utterance, \n",
    "                             goal_location, goal_type): \n",
    "        ## Modifier la norme u, il s'agit de l'output\n",
    "        #du réseau, non pas de la position !\n",
    "        with tf.name_scope(\"reward_computation\"):\n",
    "            r1 = tf.reshape(tf.square(tf.norm(goal_agent_pos - goal_location, axis = 0)), [1, self.batch_size])\n",
    "            r2 = tf.reshape(tf.square(tf.norm(goal_agent_gaze - goal_location, axis = 0)), [1, self.batch_size])\n",
    "            utt_norm = tf.square(tf.norm(new_utterance, axis = 0))\n",
    "            u_norm = tf.square(tf.norm(tf.concat([output_velocity, output_gaze], axis = 0), axis = 0))\n",
    "            vec = tf.concat([r1, r2, tf.zeros([1,self.batch_size], tf.float32)], axis = 0)\n",
    "            v1 = tf.reduce_sum(tf.multiply(vec, goal_type), axis = 0)\n",
    "            r = -(v1 + utt_norm + u_norm)\n",
    "        return r\n",
    "    \n",
    "    def compute_new_state(self, tensor_new_utterance, tensor_new_velocity, tensor_new_gaze, tensor_memory_delta, \n",
    "                          tensor_memory_delta_last, tensor_pos, tensor_velocity, tensor_memory, tensor_memory_last):\n",
    "        ## ADD THE FORCES TO THE NEW VELOCITY !!\n",
    "        ## ADD GAUSSIAN NOISE TO THE MEMORY UPDATE !\n",
    "        new_pos = tensor_pos + tf.multiply(tensor_velocity,self.time_delta)\n",
    "        new_velocity = (tf.multiply(tensor_velocity, self.damping_coeff) + \n",
    "                                                  tf.multiply(tensor_new_velocity, self.time_delta))\n",
    "        new_gaze = tensor_new_gaze\n",
    "        new_memory = tensor_memory + tensor_memory_delta\n",
    "        new_memory_last = tensor_memory_last + tensor_memory_delta_last\n",
    "        new_utterance = tensor_new_utterance\n",
    "        \n",
    "        return new_pos, new_velocity, new_gaze, new_utterance, new_memory, new_memory_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48 ms\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, name, name_target, vocabulary_size = 20, batch_size = 1024, env_dim = 2, goal_size = 8, \n",
    "                 memory_size = 32, time_delta = 0.1, nb_actions = 3, damping_coeff = 0.5):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.name_target = name_target\n",
    "        self.env_dim = env_dim\n",
    "        self.memory_size = memory_size\n",
    "        self.name = name\n",
    "        self.goal_size = goal_size\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "        #with tf.variable_scope(\"agent\" + str(self.name)):\n",
    "        #    self.pos = tf.placeholder(tf.float32, [self.env_dim, self.batch_size]) \n",
    "        #    self.velocity = tf.placeholder(tf.float32, [self.env_dim, self.batch_size])\n",
    "        #    self.gaze = tf.placeholder(tf.float32, [self.env_dim, self.batch_size])\n",
    "        #    self.utterance = tf.placeholder(tf.float32, [self.vocabulary_size, self.batch_size])\n",
    "\n",
    "        #   self.memory = tf.placeholder(tf.float32, [self.memory_size,self.batch_size])\n",
    "\n",
    "        #   self.memory_last = tf.placeholder(tf.float32, [self.memory_size, self.batch_size])\n",
    "    \n",
    "        #    self.tensor_goal_location = tf.placeholder(tf.float32, [self.env_dim, self.batch_size])\n",
    "        #    self.tensor_goal_type = tf.placeholder(tf.float32, [self.nb_actions, self.batch_size])\n",
    "        #    self.col = tf.placeholder(tf.float32, [3, 1])\n",
    "                                            \n",
    "    def get_position(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def get_velocity(self):\n",
    "        return self.velocity\n",
    "    \n",
    "    def get_gaze(self):\n",
    "        return self.gaze\n",
    "\n",
    "    def get_utterance(self):\n",
    "        return self.utterance\n",
    "                                        \n",
    "    def get_memory(self):\n",
    "        return self.memory\n",
    "    \n",
    "    def get_memory_last(self):\n",
    "        return self.memory_last\n",
    "    \n",
    "    def get_phys_state(self):\n",
    "        return (self.get_position(), self.get_velocity(), self.get_gaze(), self.get_col)\n",
    "    \n",
    "    def get_name_target(self):\n",
    "        return self.name_target\n",
    "    \n",
    "    def get_goal(self, other_ags):\n",
    "        other_agents = [other_ags[i].get_color() for i in self.name_target[0, :]]\n",
    "        colors = tf.concat(other_agents, axis = 1)\n",
    "        return tf.concat([self.tensor_goal_type, self.tensor_goal_location, colors], axis = 0)\n",
    "          \n",
    "    def get_all_iterations_variables(self):\n",
    "        return self.pos, self.velocity, self.gaze, self.utterance, self.memory, self.memory_last\n",
    "    \n",
    "    def get_color(self):\n",
    "        return self.col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "class Environment:\n",
    "    # Use this class to instantiate an environment on N batches. All batches share the same structure, but not not the\n",
    "    # same goals.\n",
    "    def __init__(self, nb_agents = 3, nb_landmarks = 0, time_delta = 0.1, env_dim = 2, batch_size = 1024,\n",
    "                 goal_type_size = 3, damping_coef = 0.5, vocabulary_size = 20):\n",
    "        self.env_dim = env_dim\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.batch_size = batch_size\n",
    "        self.goal_type_size = goal_type_size\n",
    "        self.goal_size = self.goal_type_size + 3 + self.env_dim\n",
    "        self.nb_agents = nb_agents\n",
    "        self.nb_landmarks = nb_landmarks\n",
    "        self.time_delta = tf.constant([time_delta])\n",
    "        self.list_agents = []\n",
    "        self.list_phys_tensors = []\n",
    "        self.list_utter_tensors = []\n",
    "        self.list_mem_tensors = []\n",
    "        self.list_mem_last_tensors = []\n",
    "        self.list_goals_tensors = []\n",
    "        self.list_velocity_tensors = []\n",
    "        self.list_gaze_tensors = []\n",
    "        self.goal_type_tensors = []\n",
    "        self.col_tensors = []\n",
    "        self.goal_location_tensors = []\n",
    "        self.damping_coeff = damping_coef\n",
    "        self.enc = OneHotEncoder(n_values=self.goal_type_size, sparse=False)\n",
    "    \n",
    "        \n",
    "    def init_agents(self):\n",
    "        for i in range(self.nb_agents):\n",
    "            ag = Agent(name = i, name_target = self.name_of_targets[i])\n",
    "            self.list_agents.append(ag)\n",
    "            self.list_phys_tensors.append(ag.get_position())\n",
    "            self.list_utter_tensors.append(ag.get_utterance())\n",
    "            self.list_velocity_tensors.append(ag.velocity)\n",
    "            self.list_gaze_tensors.append(ag.velocity)\n",
    "            self.goal_location_tensors.append(ag.tensor_goal_location)\n",
    "            self.goal_type_tensors.append(ag.tensor_goal_type)\n",
    "            self.col_tensors.append(ag.col)\n",
    "            self.list_mem_tensors.append(ag.get_memory())\n",
    "            self.list_mem_last_tensors.append(ag.get_memory_last())\n",
    "            \n",
    "    def init_goals_tensors(self):\n",
    "        for ag in self.list_agents: \n",
    "            self.list_goals_tensors.append(ag.get_goal(self.list_agents))\n",
    "\n",
    "    def random_env_init(self):\n",
    "        self.name_of_targets = []\n",
    "        self.p = []\n",
    "        self.v = []\n",
    "        self.g = []\n",
    "        self.go = []\n",
    "        self.co = []\n",
    "        self.gl = []\n",
    "        self.utter = []\n",
    "        self.memory = []\n",
    "        self.memory_last = []\n",
    "        for i in range(self.nb_agents):\n",
    "            #self.name_of_targets.append(np.random.randint(0, self.nb_agents, (1, self.batch_size)))\n",
    "            self.name_of_targets.append((np.ones((1, self.batch_size))*i))\n",
    "            bound = 5\n",
    "            self.p.append(np.round(np.random.uniform(-bound, bound, [2, self.batch_size]), 2))\n",
    "            self.gl.append(np.round(np.random.uniform(-bound, bound, [2, self.batch_size]), 2))\n",
    "            self.v.append(np.zeros([self.env_dim, self.batch_size]))\n",
    "            self.g.append(np.zeros([self.env_dim, self.batch_size]))\n",
    "            J = np.random.choice(self.goal_type_size, self.batch_size)\n",
    "            self.go.append(np.transpose(self.enc.fit_transform(J.reshape(-1,1))))\n",
    "            #self.go.append(np.transpose(np.array([[1,0, 0] for i in range(self.batch_size)])))\n",
    "            self.co.append(np.random.uniform(0, 255, [3, 1]))\n",
    "            \n",
    "            self.utter.append(np.zeros([20, self.batch_size]))\n",
    "            self.memory.append(np.zeros([32, self.batch_size]))\n",
    "            self.memory_last.append(np.zeros([32, self.batch_size]))\n",
    "            \n",
    "            \n",
    "        \n",
    "        #self.init_agents()\n",
    "        #self.init_goals_tensors()   \n",
    "        return self.p, self.gl, self.v, self.g, self.go, self.utter, self.co, self.memory, self.memory_last, self.name_of_targets\n",
    "    \n",
    "    def get_placeholders(self):\n",
    "        return [self.list_agents, self.list_phys_tensors, self.list_utter_tensors,self.list_velocity_tensors, \n",
    "        self.list_gaze_tensors, self.goal_location_tensors, self.goal_type_tensors, self.col_tensors, \n",
    "        self.list_mem_tensors, self.list_mem_last_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 274 ms\n"
     ]
    }
   ],
   "source": [
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, nb_agents, nb_landmarks, time_horizon, batch_size = 1024, time_delta = 0.1):\n",
    "        #tf.reset_default_graph()\n",
    "        self.time_horizon = time_horizon\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_agents = nb_agents\n",
    "        self.nb_landmarks = nb_landmarks\n",
    "        self.time_delta = time_delta\n",
    "        \n",
    "        self.instantiate_environment()\n",
    "        self.define_placeholders()\n",
    "        \n",
    "    def instantiate_environment(self):## ADD THE AUXILIARY REWARDS !!!!!\n",
    "        self.env = Environment(nb_agents = self.nb_agents)\n",
    "        self.policy = Policy(self.nb_agents, self.nb_landmarks, self.env.goal_size)\n",
    "     \n",
    "    def define_placeholders(self):\n",
    "        (self.h_pos, self.h_utter, self.h_velocity, self.h_gaze, self.h_goal_location, self.h_goal_type, \n",
    "         self.h_col, self.h_mem, self.h_last_mem, self.h_name_targets) = self.policy.get_placeholders()\n",
    "        self.list_goals = self.policy.get_list_goals(self.h_name_targets)\n",
    "        \n",
    "    def create_feed_dict(self, p, gl, v, g, go, utter, co, memory, memory_last, name_targets):\n",
    "        init_values = p + gl + v + g + go + utter + co + memory + memory_last + name_targets\n",
    "        placeholders_list = (self.h_pos + self.h_goal_location + self.h_velocity + self.h_gaze + self.h_goal_type + \n",
    "                             self.h_utter + self.h_col + self.h_mem + self.h_last_mem + self.h_name_targets)\n",
    "        feed_dict = {a:b for a,b in zip(placeholders_list, init_values)}\n",
    "        return feed_dict\n",
    "        \n",
    "    def train_batch(self, step_number, sess):\n",
    "        self.overall_res = []\n",
    "        self.all_env = []\n",
    "        t = tf.constant(1)\n",
    "        return_sofar = tf.constant(0.0)\n",
    "        self.suite_pos = []\n",
    "        arguments = [self.h_pos, self.h_gaze, self.h_utter, self.h_velocity, self.list_goals, self.h_mem, self.h_last_mem, \n",
    "                     self.h_goal_location, self.h_name_targets, self.h_goal_type, return_sofar, t]\n",
    "        result = tf.while_loop(self.condition, self.body, loop_vars = arguments)\n",
    "        #step = optimizer.minimize(-result[-2])\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        #starter_learning_rate = 0.00001 #0.97\n",
    "        learning_rate = 0.00001\n",
    "        #learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "        #                                   1, 0.8, staircase=True)\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "        tf.summary.scalar('accuracy', result[-2])\n",
    "        tf.summary.scalar('learning rate', learning_rate)\n",
    "        grads = optimizer.compute_gradients(-result[-2])\n",
    "        step = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "        for index, grad in enumerate(grads):\n",
    "            tf.summary.histogram(\"{}-grad\".format(grads[index][1].name), grads[index])\n",
    "        \n",
    "        print(\"Initializing variables\")\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        merged = tf.summary.merge_all()\n",
    "        file_writer = tf.summary.FileWriter('Summary/train',sess.graph)\n",
    "        sess.run(init_op)\n",
    "        sess.graph.finalize()\n",
    "        print(\"Start running\")\n",
    "        start = datetime.now()\n",
    "        for j in range(step_number):\n",
    "            p, gl, v, g, go, utter, co,  memory, memory_last, name_targets = self.env.random_env_init()\n",
    "            self.all_env.append([p,gl,v,g,go,utter,co,memory,memory_last,name_targets])\n",
    "            self.feed_dict = self.create_feed_dict(p, gl, v, g, go, utter, co, memory, memory_last, name_targets)\n",
    "            list_output = sess.run([step] + result, self.feed_dict)\n",
    "            #file_writer.add_summary(list_output[1], j)\n",
    "            self.overall_res.append(list_output)\n",
    "            if j%10 == 0:\n",
    "                final_pos = list_output[1]\n",
    "                final_gaze = list_output[2]\n",
    "                final_dist = [np.sqrt(np.sum((pos - gl[k])**2, axis = 0)) for k,pos in enumerate(final_pos)]\n",
    "                final_gaze = [np.sqrt(np.sum((gaze - gl[k])**2, axis = 0)) for k,gaze in enumerate(final_gaze)]\n",
    "                zeros = np.zeros(self.batch_size)\n",
    "                self.overall_res.append(list_output)\n",
    "                print(\"iteration \" + str(j))\n",
    "                print(list_output[-2])\n",
    "                for l in range(self.nb_agents):\n",
    "                    r = np.stack([final_dist[l], final_gaze[l], zeros], axis = 0)\n",
    "                    dist_obj = np.sum(np.multiply(r, go[l]), axis = 0)\n",
    "                    final_dist_mean = np.mean(dist_obj)\n",
    "                    final_dist_median = np.median(dist_obj)\n",
    "                    print(\"-- Agent\" + str(l))\n",
    "                    print(\"---final distance mean: \" + str(final_dist_mean))\n",
    "                    print(\"---final distance median:\" + str(final_dist_median))\n",
    "                    \n",
    "                print(\"computing time\")\n",
    "                print(datetime.now() - start)\n",
    "                start = datetime.now()\n",
    "                print(\"\\n\")\n",
    "        \n",
    "        return self.overall_res\n",
    "    \n",
    "    # Structure of args [h_pos, h_utterance, h_velocity, list_goals, h_mem, h_mem_last, return_sofar, t]\n",
    "    def body(self, h_pos, h_gaze, h_utterance, h_velocity, list_goals, h_mem, h_mem_last, h_goal_locations, h_name_targets, \n",
    "             h_goal_type, return_sofar, t):\n",
    "        list_moves, list_gazes, list_utterances, list_delta_mem, list_last_delta_mem =self.policy.compute_output(\n",
    "                h_pos, h_utterance, list_goals, h_mem, h_mem_last)\n",
    "        new_pos_agents = []\n",
    "        new_utterance_agents = []\n",
    "        new_velocity_agents = []\n",
    "        new_gaze_agents = []\n",
    "        new_h_mem_agents = []\n",
    "        new_h_mem_last_agents = []\n",
    "        for i in range(self.env.nb_agents):\n",
    "            output_utterance = list_utterances[i]\n",
    "            output_velocity = list_moves[i]\n",
    "            output_gaze = list_gazes[i]\n",
    "            memory_delta = list_delta_mem[i]\n",
    "            memory_delta_last = list_last_delta_mem[i]\n",
    "            \n",
    "            new_pos, new_velocity, new_gaze, new_utterance, new_memory, new_memory_last = self.policy.compute_new_state(\n",
    "                output_utterance, output_velocity, output_gaze, memory_delta, memory_delta_last, h_pos[i], \n",
    "                h_velocity[i], h_mem[i], h_mem_last[i])\n",
    "            \n",
    "            new_pos_agents.append(new_pos)\n",
    "            new_velocity_agents.append(new_velocity)\n",
    "            new_utterance_agents.append(new_utterance)\n",
    "            new_gaze_agents.append(new_gaze)\n",
    "            new_h_mem_agents.append(new_memory)\n",
    "            new_h_mem_last_agents.append(new_memory_last)\n",
    "\n",
    "        new_reward_batch = self.policy.compute_reward_agents(new_pos_agents, new_gaze_agents, new_velocity_agents,\n",
    "                                              new_utterance_agents, h_goal_locations, h_name_targets, h_goal_type)\n",
    "        \n",
    "\n",
    "        return_sofar += new_reward_batch\n",
    "        t += 1\n",
    "        \n",
    "        return [new_pos_agents, new_gaze_agents, new_utterance_agents, new_velocity_agents, list_goals, new_h_mem_agents, new_h_mem_last_agents, \n",
    "        h_goal_locations, h_name_targets, h_goal_type, return_sofar, t]\n",
    "        \n",
    "    # Structure of args [h_pos, h_utterance, h_velocity, list_goals, h_mem, h_mem_last, return_sofar, t]\n",
    "    def condition(self, h_pos, h_gaze, h_utterance, h_velocity, list_goals, h_mem, h_mem_last, h_goal_locations, \n",
    "                  h_name_targets, h_goal_type, return_sofar, t):\n",
    "        return tf.less(t, self.time_horizon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name learning rate is illegal; using learning_rate instead.\n",
      "INFO:tensorflow:Summary name phys_variable/weight_0:0-grad is illegal; using phys_variable/weight_0_0-grad instead.\n",
      "INFO:tensorflow:Summary name phys_variable/weight_1:0-grad is illegal; using phys_variable/weight_1_0-grad instead.\n",
      "INFO:tensorflow:Summary name phys_variable/weight_2:0-grad is illegal; using phys_variable/weight_2_0-grad instead.\n",
      "INFO:tensorflow:Summary name phys_variable/bias_0:0-grad is illegal; using phys_variable/bias_0_0-grad instead.\n",
      "INFO:tensorflow:Summary name phys_variable/bias_1:0-grad is illegal; using phys_variable/bias_1_0-grad instead.\n",
      "INFO:tensorflow:Summary name phys_variable/bias_2:0-grad is illegal; using phys_variable/bias_2_0-grad instead.\n",
      "INFO:tensorflow:Summary name Variable:0-grad is illegal; using Variable_0-grad instead.\n",
      "INFO:tensorflow:Summary name com_variable/com_net_weight_0:0-grad is illegal; using com_variable/com_net_weight_0_0-grad instead.\n",
      "INFO:tensorflow:Summary name com_variable/com_net_weight_1:0-grad is illegal; using com_variable/com_net_weight_1_0-grad instead.\n",
      "INFO:tensorflow:Summary name com_variable/com_net_weight_2:0-grad is illegal; using com_variable/com_net_weight_2_0-grad instead.\n",
      "INFO:tensorflow:Summary name com_variable_1/com_net_bias_0:0-grad is illegal; using com_variable_1/com_net_bias_0_0-grad instead.\n",
      "INFO:tensorflow:Summary name com_variable_1/com_net_bias_1:0-grad is illegal; using com_variable_1/com_net_bias_1_0-grad instead.\n",
      "INFO:tensorflow:Summary name com_variable_1/com_net_bias_2:0-grad is illegal; using com_variable_1/com_net_bias_2_0-grad instead.\n",
      "INFO:tensorflow:Summary name weight_mem_com:0-grad is illegal; using weight_mem_com_0-grad instead.\n",
      "INFO:tensorflow:Summary name bias_mem_com:0-grad is illegal; using bias_mem_com_0-grad instead.\n",
      "INFO:tensorflow:Summary name Variable_1:0-grad is illegal; using Variable_1_0-grad instead.\n",
      "INFO:tensorflow:Summary name last_variable/lastnet_weight_0:0-grad is illegal; using last_variable/lastnet_weight_0_0-grad instead.\n",
      "INFO:tensorflow:Summary name last_variable/last_net_weight_1:0-grad is illegal; using last_variable/last_net_weight_1_0-grad instead.\n",
      "INFO:tensorflow:Summary name last_variable/last_net_weight_2:0-grad is illegal; using last_variable/last_net_weight_2_0-grad instead.\n",
      "INFO:tensorflow:Summary name last_variable_1/last_net_bias_0:0-grad is illegal; using last_variable_1/last_net_bias_0_0-grad instead.\n",
      "INFO:tensorflow:Summary name last_variable_1/last_net_bias_1:0-grad is illegal; using last_variable_1/last_net_bias_1_0-grad instead.\n",
      "INFO:tensorflow:Summary name last_variable_1/last_net_bias_2:0-grad is illegal; using last_variable_1/last_net_bias_2_0-grad instead.\n",
      "INFO:tensorflow:Summary name weight_mem_last:0-grad is illegal; using weight_mem_last_0-grad instead.\n",
      "INFO:tensorflow:Summary name bias_mem_last:0-grad is illegal; using bias_mem_last_0-grad instead.\n",
      "Initializing variables\n",
      "Start running\n",
      "iteration 0\n",
      "-51510.9\n",
      "-- Agent0\n",
      "---final distance mean: 8.88185455109\n",
      "---final distance median:5.16913724217\n",
      "-- Agent1\n",
      "---final distance mean: 7.0906193132\n",
      "---final distance median:4.81152501838\n",
      "computing time\n",
      "0:00:11.767859\n",
      "\n",
      "\n",
      "iteration 10\n",
      "-63175.6\n",
      "-- Agent0\n",
      "---final distance mean: 7.83335546988\n",
      "---final distance median:4.70323405741\n",
      "-- Agent1\n",
      "---final distance mean: 8.87952539693\n",
      "---final distance median:5.53510759969\n",
      "computing time\n",
      "0:01:55.437817\n",
      "\n",
      "\n",
      "iteration 20\n",
      "-16760.9\n",
      "-- Agent0\n",
      "---final distance mean: 4.115366263\n",
      "---final distance median:3.7892776011\n",
      "-- Agent1\n",
      "---final distance mean: 5.34159926299\n",
      "---final distance median:4.24532260834\n",
      "computing time\n",
      "0:02:12.746800\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "exp = Experiment(2, 0, 50)\n",
    "with tf.Session() as sess:\n",
    "    l = exp.train_batch(5000, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.27 s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "r = [exp.overall_res[i][-2] for i in range(len(exp.overall_res))]\n",
    "dr = pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAFkCAYAAADhSHsMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4d0072ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "#pd.rolling_mean(dr, 60).plot()\n",
    "plt.plot(r)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-182937.19,\n",
       " -182937.19,\n",
       " -58116.5,\n",
       " -15716.643,\n",
       " -74588.805,\n",
       " -60870.434,\n",
       " -94771.922,\n",
       " -58028.039,\n",
       " -31981.943,\n",
       " -30729.436,\n",
       " -27055.594,\n",
       " -46303.738,\n",
       " -46303.738,\n",
       " -27625.006,\n",
       " -9790.7119,\n",
       " -68469.773,\n",
       " -16773.783,\n",
       " -23215.176,\n",
       " -10358.177,\n",
       " -47120.938,\n",
       " -10080.265,\n",
       " -13030.263,\n",
       " -22157.701,\n",
       " -22157.701,\n",
       " -18941.359,\n",
       " -13098.636,\n",
       " -9756.0957,\n",
       " -13227.911,\n",
       " -19204.219,\n",
       " -9579.959,\n",
       " -19424.828,\n",
       " -24096.383,\n",
       " -11794.164,\n",
       " -9276.3633,\n",
       " -9276.3633,\n",
       " -15492.31,\n",
       " -13280.396,\n",
       " -9519.8477,\n",
       " -9189.4082,\n",
       " -11323.009,\n",
       " -9812.6641,\n",
       " -30201.002,\n",
       " -10629.354,\n",
       " -11715.265,\n",
       " -9973.3242,\n",
       " -9973.3242,\n",
       " -10271.856,\n",
       " -11241.281,\n",
       " -11192.56,\n",
       " -10017.45,\n",
       " -12653.22,\n",
       " -9235.8359,\n",
       " -10840.249,\n",
       " -19158.432,\n",
       " -9851.3223,\n",
       " -9631.9766,\n",
       " -9631.9766,\n",
       " -9319.8818,\n",
       " -10204.132,\n",
       " -11105.599,\n",
       " -9811.1533,\n",
       " -15095.022,\n",
       " -16264.184,\n",
       " -9364.1768,\n",
       " -22028.66,\n",
       " -11208.357,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.4 ms\n"
     ]
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4]\n",
      " [2 1]]\n",
      "time: 543 ms\n"
     ]
    }
   ],
   "source": [
    "e = tf.constant([[1, 2], [3, 4]])\n",
    "g = tf.gather_nd(e, [tf.constant([[1, 0], [1, 1]]), tf.constant([[0, 1], [0, 0]])])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.38 ms\n"
     ]
    }
   ],
   "source": [
    "e = tf.constant([[0, 1, 2], [1, 0, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_16:0' shape=(2, 3) dtype=int32>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.49 ms\n"
     ]
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 42.1 ms\n"
     ]
    }
   ],
   "source": [
    "np.median([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-87063.805,\n",
       " -87063.805,\n",
       " -42613.492,\n",
       " -95466.906,\n",
       " -51702.867,\n",
       " -119452.19,\n",
       " -54361.605,\n",
       " -45506.168,\n",
       " -45355.227,\n",
       " -54363.707,\n",
       " -50592.652,\n",
       " -64282.707,\n",
       " -64282.707,\n",
       " -44239.941,\n",
       " -42706.688,\n",
       " -48918.352,\n",
       " -31315.318,\n",
       " -12464.795,\n",
       " -16229.566,\n",
       " -17335.041,\n",
       " -34374.93,\n",
       " -33092.418,\n",
       " -33758.09,\n",
       " -33758.09,\n",
       " -23054.051,\n",
       " -26396.902,\n",
       " -23441.33,\n",
       " -13553.037,\n",
       " -16636.049,\n",
       " -22000.59,\n",
       " -15910.657,\n",
       " -17852.219,\n",
       " -22044.727,\n",
       " -16584.189,\n",
       " -16584.189,\n",
       " -13721.631,\n",
       " -17386.037,\n",
       " -17495.488,\n",
       " -17994.559,\n",
       " -15598.205,\n",
       " -9668.4219,\n",
       " -18942.807,\n",
       " -17743.613,\n",
       " -9740.2227,\n",
       " -14156.621,\n",
       " -14156.621,\n",
       " -10293.151,\n",
       " -12772.105,\n",
       " -14022.875,\n",
       " -12508.376,\n",
       " -13253.797,\n",
       " -13280.086,\n",
       " -9958.9531,\n",
       " -6889.3926,\n",
       " -9030.5986,\n",
       " -14340.563,\n",
       " -14340.563,\n",
       " -11904.121,\n",
       " -10879.037,\n",
       " -7775.1309,\n",
       " -12467.575,\n",
       " -12387.378,\n",
       " -11923.172,\n",
       " -11323.783,\n",
       " -11144.868,\n",
       " -10955.39,\n",
       " -8239.79,\n",
       " -8239.79,\n",
       " -8856.7529,\n",
       " -8973.5801,\n",
       " -10009.217,\n",
       " -8228.1475,\n",
       " -9959.7656,\n",
       " -8921.8408,\n",
       " -9621.7822,\n",
       " -8656.2256,\n",
       " -8147.7202,\n",
       " -9009.5645,\n",
       " -9009.5645,\n",
       " -8581.2041,\n",
       " -9847.1289,\n",
       " -8783.6055,\n",
       " -8135.1001,\n",
       " -8027.188,\n",
       " -8681.8428,\n",
       " -7703.0547,\n",
       " -10157.399,\n",
       " -8117.313,\n",
       " -8001.7373,\n",
       " -8001.7373,\n",
       " -8407.6816,\n",
       " -7545.3403,\n",
       " -8257.0498,\n",
       " -8077.3325,\n",
       " -9683.75,\n",
       " -8234.916,\n",
       " -8465.2119,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.5 ms\n"
     ]
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-a268cdbdf94a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverall_res\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-206-a268cdbdf94a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverall_res\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gl' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.4 ms\n"
     ]
    }
   ],
   "source": [
    "d = [np.mean(np.sqrt(np.sum((pos[1] - gl[0])**2, axis = 0))) for pos in exp.overall_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  1.,  1.,  1.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.51 ms\n"
     ]
    }
   ],
   "source": [
    "n_samples = 20\n",
    "n_classes = 3\n",
    "J = np.random.choice(n_classes, n_samples)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(n_values=n_classes, sparse=False)\n",
    "np.transpose(enc.fit_transform(J.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "time: 2.03 ms\n"
     ]
    }
   ],
   "source": [
    "for i, k in enumerate(zip([1, 2, 3], [4, 5, 6], [7, 8, 9])):\n",
    "    print(np.dot(k, [0, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  2.,  2.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.02 ms\n"
     ]
    }
   ],
   "source": [
    "np.ones((1, 3))*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
